\documentclass[12pt, russian]{article}

\usepackage[russian]{babel}
\usepackage[a4paper,left=10mm,right=10mm, top=10mm,bottom=10mm,bindingoffset=0cm]{geometry}
\usepackage{amsfonts,amssymb,amsmath}
\usepackage{nopageno}
\usepackage{cmap}
\usepackage{amsthm}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}

\begin{document}

\newtheorem{lemma}{Лемма}
\newtheorem{theorem}{Теорема} 
\newtheorem{mydef}{Определение}
\newcommand{\probcov}{\xrightarrow{\text{п.н.}}}
\newcommand{\alprob}{\text{ п.н.}}
\newcommand{\integr}[1]{\int\limits_A{{#1}(\omega)\cdot\mathbb{P}(d\omega)}}
\newcommand{\expon}[1]{\exp{\left\{{#1}\right\}}}
\newcommand{\proj}[2]{\text{proj}_{#1}{#2}}

\begin{center}
	\begin{Huge}
		Математическая статистика
	\end{Huge}
\end{center}

\subsubsection*{1. Статистическая модель. Теорема Гливенко-Кантелли и ее использование для обоснования метода подстановки. Иллюстрация метода подстановки на примерах выборочных моментов и квантилей, их сходимость п.н.}

\begin{mydef}
Вероятностное пространство - тройка $(\Omega, \mathfrak{F}, \mathbb{P})$. 
Здесь $\Omega$ --- некоторое множество({\it элементарные исходы}), $\mathfrak{F}$ --- некоторая $\sigma$-алгебра его подмножеств({\it случайных событий}), $\mathbb{P}$ --- $\sigma$-аддитивная мера, определенная на $\sigma$-алгебре событий  ({\it вероятность}) и нормированная
$$ \mathbb{P}(\Omega) = 1 $$
\end{mydef}

\begin{mydef}
Борелевской $\sigma$-алгеброй $\mathfrak{B}(X)$ топологического пространства $(X, \tau)$ называется минимальная $\sigma$-алгебра, содержащая все множества из $\tau$ (все открытые подмножества). Нужный нам случай --- $\mathfrak{B}(\mathbb{R}^n)$
\end{mydef}

\begin{mydef}
Случайная величина --- измеримая функция на вероятностном пространстве, то есть такая функция $\xi : \Omega \rightarrow \mathbb{R}$, что $\forall X\in\mathfrak{B}(\mathbb{R})$ $\xi^{-1}(X)\in\mathfrak{F}$.
\end{mydef}

Про статистические модели: у нас есть некоторое наблюдение $X$ --- {\it статистический материал.} Мы некотором образом выяснили всевозможные мыслимые значения $X$. Они образуют некоторое множество\footnote{а если $X$ --- произвольное множество? :)} $\mathfrak{X}$ --- {\it пространство наблюдений, выборочное пространство, генеральную совокупность.}\footnote{лучше тут сказать, что выборочное пространство --- это пара $(\mathfrak{X}, \mathfrak{F})$, где $\mathfrak{F}$ --- $\sigma$-алгебра событий, на которой мы задаем вероятности выпадения $X$} $X$ выбирается из $\mathfrak{X}$ некоторым способом, но a priopi мы не знаем, как именно. Хотя у нас и могут быть некоторые изначальные предположения на этот счет. Итак, на $\mathfrak{X}$ задано некоторое семество вероятностных мер $\mathcal{P}$. 

\begin{mydef}
Распределение наблюдения $X$ --- мера $\mathbb{P}_X(A) = \mathbb{P}(\omega : X(\omega)\in A)$. Известно, что $\mathbb{P}_X \in \mathcal{P}$.
\end{mydef}

Мы рассматриваем случай параметрического семейства:
$$ \mathcal{P} = \{\mathbb{P}_{\theta},\,\theta\in\Theta \} $$

\begin{mydef}
Статистическая модель --- тройка $(\mathfrak{X}, \mathfrak{F}, \mathcal{P})$. Здесь $\mathfrak{F}$ --- $\sigma$-алгебра множеств, измеримых относительно $\mathcal{P}$.
\end{mydef}

У нас в основном будут встречаться модели, где $\mathfrak{X} = \mathbb{R}^n,\,\mathfrak{F} = \mathfrak{B}(\mathbb{R}^n)$.

\begin{mydef}
Функция распределения случайной величины $\xi$ --- функция $F_{\xi}(x) = \mathbb{P}(\xi \leq x)$
\end{mydef}

Далее везде мы будем придерживаться единых обозначений для плотности случайных величин. $f_{\xi}$ --- плотность случайной величины $\xi$. Возможны основные случаи:
\begin{enumerate}
\item $\xi$ дискретна --- принимает значений из, условно, $\mathbb{Z}$. Тогда
$$ f_{\xi}(k) = \mathbb{P}(\xi = k) $$

\item $\xi$ абсолютно непрерывна --- $F_{\xi}\in AC(\mathbb{R})$. В этом случае плотность можно определить просто:\footnote{На самом деле, вполне можно сказать, что плотность абсолютно непрерывной величины --- это не функция, а элемент пространства $L_1(\mathbb{R})$, потому как единственное, что от нее нужно --- возможность взятия лебеговского интеграла. Функция распределения, как абсолютно непрерывная, будет почти всюду дифференциируема, а значит, ее производную можно определить как элемент $L_1(\mathbb{R})$.}
$$ f_{\xi}(x) = \frac{dF_{\xi}}{dx}(x)$$

\end{enumerate}

Вернемся к статистике. Пусть $(X_1, \ldots X_n)$ --- выборка из распределения величины $X$, определенной на пространстве $\Omega$. Обозначим функцию распределения $X$ через $F$. 

\begin{mydef}
Эмпирическая (выборочная) функция распределения --- функция $\hat{F} : \Omega \rightarrow \mathbb{R}$, определяемая следующим образом:
$$ \hat{F}(x) = \frac{1}{n} \sum\limits_{i=1}^n{\mathbf{1}_{X_i \leq x}} $$ 
\end{mydef}

\begin{theorem}[Гливенко] Пусть $(X_1, \ldots X_n)$ --- выборка из распределения величины $X$ с функцией распределения $F(x)$. Тогда
$$ \mathbb{P}\left( \lim_{n \to \infty} \sup_{x \in \mathbb{R}} \left| \hat{F}_n(x) - F(x) \right| = 0 \right) = 1$$
\end{theorem}
\begin{proof}

Рассмотрим 2 случая:

\begin{enumerate}
\item{$F(x)$ непрерывна}

$\forall N \in \mathbb{N}$ выберем $-\infty = z_0 < z_1 < \ldots < z_N = \infty$ так, чтобы $F(z_k) = \frac{k}{N}, k = 0,1,\ldots N$. Пусть $x\in [z_k, z_{k+1})$. $\hat{F}_n(x)$ и $F(x)$ неубывают, а значит
$$ \hat{F}_n(x) - F(x) \leq \hat{F}_n(z_{k+1}) - F(z_k) = \hat{F}_n(z_{k+1}) - F(z_{k+1}) + \frac{1}{N} \leq \max_k{\left| \hat{F}_n(z_{k}) - F(z_{k}) \right|} + \frac{1}{N}$$
$$ \hat{F}_n(x) - F(x) \geq \hat{F}_n(z_{k}) - F(z_{k + 1}) = \hat{F}_n(z_{k}) - F(z_{k}) - \frac{1}{N} \geq - \max_k{\left| \hat{F}_n(z_{k}) - F(z_{k}) \right|} - \frac{1}{N}$$
Отсюда
$$ \sup_{x \in \mathbb{R}} \left| \hat{F}_n(x) - F(x) \right| \leq \max_{0 \leq k \leq N}{\left| \hat{F}_n(z_{k}) - F(z_{k}) \right|} + \frac{1}{N} $$
Фиксируем $z_k$ и рассмотрим события $A_k = \{\omega : \hat{F}_n(x) \rightarrow F(x) \}$. Напомним:
$$ \hat{F}_n(z_k) = \frac{1}{n} \sum\limits_{i=1}^n{\mathbf{1}_{X_i \leq z_k}} $$
По УЗБЧ\footnote{Усиленный закон больших чисел: среднее арифметическое последовательности независимых одинаково распределенных случайных величин сходится к математическому ожиданию с вероятностью $1$.} имеем
$$ \hat{F}_n(z_k) \probcov F(z_k) \Longrightarrow \forall k \quad \mathbb{P}(A_k) = 1$$
Для события $A = \bigcup\limits_{k = 1}^N{A_k}$ получаем $\mathbb{P}(A) = 1$. Обозначим $\varepsilon = \frac{1}{N}$.
$$ \forall \omega \in A \,\, \exists n_0 = n_0(\omega):\,\forall n\geq n_0 \,\, \max_k{\left| \hat{F}_n(z_{k}) - F(z_{k}) \right|} < \varepsilon \Longrightarrow $$
$$ \sup_{x \in \mathbb{R}} \left| \hat{F}_n(x) - F(x) \right| < 2\varepsilon \Longrightarrow \sup_{x \in \mathbb{R}} \left| \hat{F}_n(x) - F(x) \right| \probcov 0$$

\item{$F(x)$ разрывна (непрерывна справа)}

В этом случае берем $R\in\mathbb{N}$, $\varepsilon = \frac{1}{R}$, набор $-\infty = y_0 < y_1 < \ldots < y_N = \infty$ с условием $F(y_{k+1} - 0) - F(y_k) < \varepsilon$ и действуем аналогично первому случаю.
\end{enumerate}
\end{proof}

\begin{mydef}
Метод подстановки --- это нечто, где фигурируют:
\begin{enumerate}
\item $G(F)$ --- неизвестный функционал.
\item Оценка сходимости $G(\hat{F}_n) \rightarrow G(F)$
\end{enumerate}
\end{mydef}

Посмотрим на примеры его применения.
\begin{enumerate}
\item Выборочные моменты

$$ E|x_1|^k < \infty, k\in \mathbb{N} \Rightarrow \nu_k = E|x_1|^k = G(F) = \int\limits_{-\infty}^{+\infty}{x^k\,dF(x)} $$
$$ \hat{\nu}_k = G(\hat{F}_n) = \int\limits_{-\infty}^{+\infty}{x^k\,d\hat{F}_n(x)} = \frac{1}{n} \sum\limits_{i=1}^n{x_i^{(k)}} $$
\begin{mydef}
$\hat{\nu}_k$ называется $k$-ым эмпирическим (выборочным) моментом выборки $(x_1, \ldots x_n)$.
\end{mydef}
Согласно УЗБЧ $ \hat{\nu}_k \probcov \nu_k $

\item Квантили
\begin{mydef}
Квантилью порядка $p\in (0,1)$ распределения $F$ называется 
$$ \zeta_p = \sup\{x:F(x)\leq p\} $$
\end{mydef}
\begin{mydef}
Медианой называется квантиль порядка $\frac{1}{2}$
\end{mydef}
Переупорядочим $(x_1, \ldots x_n)$ в порядке возрастания. 
\begin{mydef}
Вариационным рядом выборки называется полученная упорядоченная последовательность
$$ x_{(1)} \leq x_{(2)} \leq \cdots x_{(n)} $$
\end{mydef}
\begin{mydef}
Величина $x_{(k)}$ называется $k$-ой порядковой статистикой выборки.
\end{mydef}

$\zeta_p = G(F), \hat{\zeta}_p = G(\hat{F}_n)$. Оказывается,
\begin{enumerate}
	\item $ \hat{\zeta}_p = x_{(l)}, \qquad l = \lfloor np \rfloor + 1$
	\item Если $F$ непрерывна и строго возрастает, то $\hat{\zeta}_p \probcov \zeta_p$
\end{enumerate}
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{2. Введение в задачу с.к. оптимального оценивания скалярного параметра: статистики, оценки, среднеквадратический риск, несмещенные оценки с равномерно наименьшей дисперсией (с.к. оптимальные оценки).}

Рассмотрим статистическую модель $\left(\mathbb{R}^n, \mathfrak{B}(\mathbb{R}^n), \mathcal{P}\right)$ и выборку $X = (X_1, \ldots X_n) \in \mathbb{R}^n$. Обозначим $\mathbb{P}_x(A) = \mathbb{P}(x \in A)$, $\mathcal{P}$ --- множество возможных распределений $X$, заданное в параметрическом виде:
$$ \mathcal{P} = \{ \mathbb{P}_\theta : \theta \in \Theta \subseteq \mathbb{R} \} $$
$\mathbb{P}_x = \mathbb{P}_\theta$ (для некоторого $\theta$), то есть $X \sim p(x, \theta) $. Требуется по известному значению вектора $X$ оценить $\theta$.
Рассмотрим некоторые примеры: 
\begin{enumerate}
\item
$X = (X_1, \ldots X_n)$ --- выборка. $X_i$ --- норсв\footnote{Здесь и далее сокращение для "Независимые одинаково распределенные случайные величины"}. $X_i \sim \mathcal{N}(\theta, \sigma^2),\,\theta\in\mathbb{R}$. В этом случае
$$ p(x, \theta) = \frac{1}{(\sqrt{2\pi}\sigma)^n} e^{-\frac{1}{2\sigma^2} \sum\limits_{i=1}^n{(x_i - \theta)^2}}$$

\item $X = (X_1, \ldots X_n)$ --- выборка. $X_i$ --- норсв, $X_1 \sim Bin(1, \theta),\,\theta\in (0,1)$ 
$$ \mathbb{P}_\theta(X_1 = y) = \theta^y \cdot (1 - \theta)^{1 - y},\qquad y\in\{ 0, 1\} \Longrightarrow $$
$$ p(x, \theta) = \theta^{\sum{x_i}} \cdot (1 - \theta)^{1 - \sum{x_i}}, \qquad x=(x_1,\ldots x_n), x_i\in\{ 0, 1\} $$ 
\end{enumerate}

\begin{mydef}
Статистика выборки --- измеримая функция $\varphi : \mathbb{R}^n \rightarrow \mathbb{R}^k$, зависящая от элементов выборки, но не зависящая от неизвестных параметров: $\varphi(X) = \varphi(X_1, \ldots X_n)$
\end{mydef}

Статистика выборки является случайной величиной. 

\begin{mydef}
Оценкой параметра $\theta$ называется произвольная одномерная статистика: $\hat{\theta}_n = \varphi(x)$
\end{mydef}

\begin{mydef}
Среднеквадратичным риском оценки $\hat{\theta}_n$ в точке $\theta$ называется величина $R_n(\hat{\theta}_n, \theta) = E_\theta(\hat{\theta}_n - \theta)^2$
\end{mydef}

\begin{mydef}
Оценка $\hat{\theta}_n$ называется оптимальной в среднем квадратичном, если
$$ \forall \theta \in \Theta \,\, \forall \tilde{\theta}_n \quad R_n(\hat{\theta}_n, \theta) \leq R_n(\tilde{\theta}_n, \theta)$$
\end{mydef}

Оценки, как правило, ищутся в следующем достаточно узком классе:

\begin{mydef}
Оценка $\varphi$ параметра $\theta$ (или функции $\tau(\theta)$) называется несмещенной, если $\forall \theta \in \Theta \,\, E_\theta\varphi(x) = \theta$ (соответственно, $\tau(\theta)$)
\end{mydef}

Для несмещенных оценок риск будет являться дисперсией: $D_\theta\hat{\tau}_n = R_n(\hat{\tau}_n, \tau(\theta))$

Итак, теперь мы хотим найти оценки с равномерно по $\theta$ минимальной дисперсией. 

Приведем интересный пример:
Пусть $X = X_1$ --- выборка из одного элемента из Пуассоновского распределения $Pois(\theta)$ с параметром $\theta$. Его плотность задается формулой 
$$ \mathbb{P}_\theta(X = k) = \frac{\theta^k}{k!} e^{-\theta} $$
Докажем, что для такой выборки невозможно несмещенно оценить величину $\tau(\theta) = \frac{1}{\theta}$. Предположим противное. Пусть $\hat{\tau}_1(X_1)$ --- несмещенная оценка. Тогда
$$ \forall\theta>0\,\,E_\theta\hat{\tau}_1(X_1) = \sum\limits_{k=0}^\infty{\hat{\tau}_1(k) \frac{\theta^k}{k!} e^{-\theta}} = \frac{1}{\theta} $$
Последнее равенство, очевидно, не может выполняться при всех положительных $\theta$

\begin{mydef}
Несмещенная оценка с конечной дисперсией $\hat{\tau}(X)$ называется оптимальной в среднем квадратичном (с.к-оптимальной), если для всякой другой несмещенной оценки с конечной дисперсией\footnote{Это важно. Несмещенные оценки с конечным вторым моментом можно разумно сравнивать только с несмещенными оценками с конечным вторым моментом} $\tilde{\tau}(X)$ выполнено
$$ \forall\theta\in\Theta\quad D_\theta\hat{\tau}(X) \leq D_\theta\tilde{\tau}(X) $$
\end{mydef}

К вопросу о вычислении величины $E_\theta$. Пусть $\mathbb{P}_\theta\sim p(y, \theta)$. Обозначим $N_p = \{x:\exists\theta: p(x, \theta) > 0 \}$ Тогда
\begin{equation*}
\begin{split}
E_\theta\varphi(X) = \int\limits_{N_p}{\varphi(x) p(x, \theta) \mu(dx)}
&=\left\{
        \begin{array}{ll}
          \int\limits_{N_p}{\varphi(x) p(x, \theta) \mu(dx)}, & \hbox{в абсолютно непрерывном случае} \\
          \sum\limits_{y_i:  \mathbb{P}_\theta (X = y_i) > 0}{\varphi (y_i) \mathbb{P}_\theta (X = y_i)}, & \hbox{в дискретном случае}
        \end{array}
      \right.
\end{split}
\end{equation*}

Функция

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{3. Неравенство Рао-Крамера в случае скалярного параметра. Случай независимых одинаково распределенных данных.}

Пусть $X\in\mathbb{R}^n$ --- наблюдение с плотностями компонент $p(x, \theta),\, \theta\in\Theta\subseteq\mathbb{R},\,\Theta$ открыто. $T(X)$ --- статистика с конечными первым и вторым моментами. Обозначим $\tau(\theta) = E_\theta T(X)$

Введем некоторые дополнительные предположения о плотности $p(x, \theta)$, называемые {\it условиями регулярности}:
\begin{enumerate}
\item Множество $N_p = {x: p(x, \theta) > 0}$ не зависит от $\theta$
\item $\forall\theta\in\Theta\,\,\forall x\in N_p$ 
$$ \exists\lambda(x, \theta) = \frac{\partial}{\partial\theta}\ln{p(x, \theta)} $$
\item Дифференциируемость: $\forall\theta\in\Theta$
\begin{enumerate}
	\item $$\exists\frac{\partial}{\partial\theta} \int\limits_{N_p}{p(x, \theta)\,\mu(dx)} =\int\limits_{N_p}{\frac{\partial}{\partial\theta}p(x, \theta)\,\mu(dx)} = 0\footnote{$\int\limits_{N_p}{p(x, \theta)\,\mu(dx)} = 1$} $$
	\item $$ \exists\frac{\partial}{\partial\theta} \int\limits_{N_p}{T(x) p(x, \theta)\,\mu(dx)} =\int\limits_{N_p}{T(x) \frac{\partial}{\partial\theta}p(x, \theta)\,\mu(dx)} = \tau'(\theta) $$
\end{enumerate}
\item 
\begin{mydef}
	Информацией по Фишеру, а, точнее, количеством информации о параметре $\theta$, содержащемся в наблюдении $X$, называется величина
	$$ I(\theta) = E_\theta\left[\frac{\partial}{\partial\theta}\ln{p(X, \theta)}\right]^2 = E_\theta \lambda^2(X, \theta) $$
\end{mydef}
Последнее свойство регулярности: 
$$ 0 < I(\theta) < \infty $$
\end{enumerate}

\begin{theorem}[\bf неравенство Крамера -- Рао]
$$ D_\theta T(X) \geq \frac{[\tau'(\theta)]^2}{I(\theta)}$$
Для несмещенных оценок параметра, когда $\tau(\theta) = \theta$ из этого следует, что 
$$ D_\theta T(X) \geq \frac{1}{I(\theta)}$$
\end{theorem}
\begin{proof}
$ $
\begin{enumerate}
\item Из третьего условия регулярности получим
$$ E_\theta\lambda(X, \theta) = \int\limits_{N_p}{\left[\frac{\partial}{\partial\theta}\ln{p(x, \theta)} \right] p(x, \theta)\,dx} = \int\limits_{N_p}{\frac{\partial}{\partial\theta}p(x, \theta)\,dx} = 0 $$
\item Аналогично  
$$ \tau'(\theta) = \int\limits_{N_p}{T(X) \left[\frac{\partial}{\partial\theta}\ln{p(x, \theta)} \right] p(x, \theta)\,dx} = E_\theta T(X)\lambda(X, \theta) = E_\theta [T(X) - \tau(\theta)]\lambda(X, \theta) $$
\item Применим неравенство Коши-Буняковского.\footnote{$ (E\xi\eta)^2 \leq E\xi E\eta $}
Положим $\xi = T(X) - \tau(\theta),\,\eta = \lambda(X, \theta)$. Получим:
$$ [\tau'(\theta)]^2 \leq I(\theta)D_\theta T(X) \Longleftrightarrow D_\theta T(X) \geq \frac{[\tau'(\theta]^2}{I(\theta)} $$
\end{enumerate}
\end{proof}

Рассмотрим теперь выборку $X = (X_1, X_2 \ldots X_n)$. Пусть $I_X(\theta)$ --- количество информации во всей выборке, а $i(\theta)$ --- в отдельном наблюдении. Рассмотрим связь между этими величинами.

\begin{lemma}
$$ I_X(\theta) = n\cdot i(\theta)$$
\end{lemma}
\begin{proof}
Совместная плотность $X = (X_1, X_2 \ldots X_n)$ равна $\prod\limits_{i=1}^n{f(X_i, \theta)},$ где $f(\cdot, \theta)$ --- плотности отдельных $X_i.$ Из этого получаем:
$$ \lambda(X, \theta) = \sum\limits_{i=1}^n{\frac{\partial}{\partial\theta} \ln{f(X_i, \theta)}} $$
Отсюда 
$$ I(X) = E_\theta \lambda^2(X, \theta) = \sum\limits_{i=1}^n{E_\theta\left[\frac{\partial}{\partial\theta} \ln{f(X_i, \theta)}\right]^2} = n\cdot i(\theta)$$
\end{proof}

Напоследок, интересная формула:
$$ I(\theta) = -E_\theta \frac{\partial^2}{\partial\theta^2}\ln{p(X, \theta)}$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{4. Экспоненциальные семейства распределений, примеры. Теоремы о достаточных и необходимых условиях равенства в неравенстве Рао-Крамера. Эффективные оценки, примеры.}

Опять вспоминаем неравенство Коши-Буняковского:
$$ (E\xi\eta)^2 \leq E\xi E\eta $$
Равенство в нем достигается тогда и только тогда, когда $\xi$ и $\eta$ линейно зависимы, то есть в случае
$$ \exists A,B,C\quad \mathbb{P}\{A\xi + B\eta + C = 0 \}= 1$$
В нашем случае $\xi = T(X) - \tau(\theta),\,\eta = \lambda(X, \theta)$ получим:
$$\exists a = a(\theta)\quad T(X) = \tau(\theta) + a(\theta)\lambda(X, \theta)$$

\begin{mydef}
Эффективная оценка --- оценка, для которой неравенство Крамера-Рао обращается в равенство.
\end{mydef}

Не для каждого семейства распределений и не для каждой функции неизвестного параметра существуют эффективные оценки. Нам хотелось бы найти условия их существования. Преобразуем предыдущее равенство.
$$ \frac{\partial}{\partial\theta} \ln{p(X, \theta)} = \frac{1}{a(\theta)}T(X) + \frac{\tau(\theta)}{a(\theta)}$$
Интегрируя, получаем (вообще говоря, не единственное) представление плотности:
$$ p(x, \theta) = \exp{\{c(\theta)T(X) + d(\theta) + S(x)}\}\cdot\mathbf{1}_{N_p}(x) $$
\begin{mydef}
Экспоненциальным семейством называется семейство распределений с плотностями вида
$$ p(x, \theta) = \exp{\{c(\theta)T(X) + d(\theta) + S(x)}\}\cdot\mathbf{1}_{N_p}(x) $$
\end{mydef}

Для экспоненциальных семейств существуют эффективные оценки для $\tau(\theta) = -\frac{d'(\theta)}{c(\theta)}$

Выборка из экспоненциального семейства также принадлежит экспоненциальному семейству с плотностью
$$ p(x_1, \ldots, x_n, \theta) = \exp{\left\{ c(\theta)\sum\limits_{i=1}^n{T(x_i)} + nd(\theta) + \sum\limits_{i=1}^n{S(x_i)} \right\}} \cdot \mathbf{1}_{N_p \times \cdots \times N_p}(x_1,\ldots,x_n) $$

Приведем простой пример: пусть имеется биномиальное распределение с плотностью 
$$ p(x, \theta) = C_n^x \theta^x (1 - \theta)^{n - x} = \exp{\left\{ x\ln{\frac{\theta}{1 - \theta}} + n\ln{(1 - \theta)} + \ln{C_n^x} \right\}} $$.
В данном случае есть эффективная оценка $\frac{X}{n}$ для параметра $\theta$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{5. Оценивание векторного параметра: несмещенные оценки с равномерно наименьшей ковариационной матрицей, векторное неравенство Рао-Крамера (без доказательства), эффективные оценки.}

Пусть $X$ --- случайный вектор, распределение которого определяется параметром $\theta$. В общем случае будем рассматривам $r$-мерный параметр $\theta\in\Theta\subseteq\mathbb{R}^r,\,\Theta$ --- некоторое открытое множество. Рассмотрим задачу оценивания $\theta$ (или функций от $\theta$).

\begin{mydef}
Случайная матрица $Z$ есть матрица, элементы $z_{ij}$ которой суть случайные величины, заданные на общем пространстве элементарных исходов, то есть имеющие совместное распределение вероятностей. 
\end{mydef}

\begin{mydef}
Математическое ожидание случайной матрицы есть $EZ = ||Ez_{ij}||$\footnote{Это не норма, а сама матрица, составленная из матожиданий}
\end{mydef}

Основные линейные свойства многомерного матожидания:
\begin{enumerate}
\item
Пусть матрицы $A, B, C$ таковы, что $\exists(AZB + C)$. Тогда
$$ E(AZB + C) = A(EZ)B + C $$
В частности, если $Y\in\mathbb{R}^n$, $A\in M_n(\mathbb{R})$ --- неслучайная матрица, $b\in\mathbb{R}^n$ --- неслучайный вектор, то
$$ E(AY + b) = A(EY) + b $$
\item
Для матриц, определенных на одном и том же вероятностном пространстве и имеющих одинаковый размер, выполнено
$$ E(Z_1 + Z_2) = EZ_1 + EZ_2 $$
\end{enumerate}

Определим еще одну важную для случайных векторов вещь. Пусть $X$ и $Y$ --- вектора, определенные на одном и том же вероятностном пространстве (но не обязательно имеющие одинаковый размер).
\begin{mydef}
Ковариационной (или дисперсионной) матрицей пары векторов $X$ и $Y$ называется матрица
$$ cov(X, Y) = E(X -EX)(Y-EY)^T = E(XY^T) - (EX)(EY)^T $$
Пусть $X = (\xi_1, \xi_2 \ldots),\,Y= (\eta_1, \eta_2\ldots)$, тогда 
$$ cov(X, Y)_{ij} = cov(\xi_i, \eta_j) $$
\end{mydef}

\begin{mydef}
Ковариационной матрицей случайного вектора называется матрица 
$$ cov(X, X) = DX = EXX^T - (EX)(EX)^T $$
Ее диагональные элементы --- дисперсии $D\xi_i$
\end{mydef}

Свойства дисперсий векторов:
\begin{enumerate}
\item Пусть $X$ --- случайный вектор, $A$ --- неслучайная матрица, $b$ --- неслучайный вектор. Тогда
$$ \exists(AX + b) \Longrightarrow D(AX + b) = A(DX)A^T$$
\item Частный случай предыдущего пункта: $X\in\mathbb{R}^n$ --- случайный вектор, $A=a^T, a\in\mathbb{R}^n$. Тогда $AX = \langle a, X\rangle$ и дисперсия выражается как
$$ D(AX) = D(a^TX) = a^T(DX)a $$
\end{enumerate}

\noindent Вернемся к задаче оценивания многомерного параметра. $\varphi:\mathbb{R}^n \rightarrow \mathbb{R}^d$ --- несмещенная оценка функции параметра $\tau(\theta) = (\tau_1(\theta),\ldots,\tau_d(\theta)),\,\theta\in\Theta\subseteq\mathbb{R}^r$, то есть $E_\theta\varphi(X) = \tau(\theta)$. Сведем задачу нахождения наилучшей оценки к одномерной. Пусть задана пара оценок $\varphi(X), \psi(X)$. Рассмотрим скалярные произведения
$$ \xi := z^T\varphi(X),\quad \eta := z^T\psi(X),\quad t(\theta) = z^T\tau(\theta),\quad z\in\mathbb{R}^d$$
$\xi$ и $\eta$ являются несмещенными одномерными оценками для $t(\theta)$, то есть
$$ E_\theta\xi = E_\theta\eta = t(\theta) $$
Признаком "качества"\,одномерной оценки является малость дисперсии. В частности, $\xi$ не хуже, чем $\eta$, если
$$ D_\theta\xi \leq D_\theta\eta \Longleftrightarrow z^T(D_\theta\varphi(X))z \leq z^T(D_\theta\psi(X))z $$
В связи с этим введем следующее определение
\begin{mydef}
Оценка $\varphi(X)$ не хуже, чем $\psi(X)$, если
$$ \forall z\in\mathbb{R}^d\quad z^T(D_\theta\varphi(X))z \leq z^T(D_\theta\psi(X))z $$
и лучше $\psi(X)$, если, к тому же
$$ \exists z_0\in\mathbb{R}^d\quad z_0^T(D_\theta\varphi(X))z_0 < z_0^T(D_\theta\psi(X))z_0 $$
\end{mydef}

$z^T(D_\theta\varphi(X))z$ и $z^T(D_\theta\psi(X))z$ являются квадратичными формами переменного $z\in\mathbb{R}^d$, причем неотрицательно определенными. Таким образом, неравенство из предыдущего определения превращается в систему поэлементных неравенств на матрицы форм:
$$ D_\theta\varphi(X) \leq D_\theta\psi(X) $$ 

Это приводит нас к следующему
\begin{mydef}
Квадратичным риском статистики $\varphi(X)$, несмещенно оценивающей $\tau(\theta)$, называется ее матрица ковариаций
$$ D_\theta\varphi(X) = E_\theta[\varphi(X) - \tau(\theta)][\varphi(X) - \tau(\theta)]^T $$
\end{mydef}

\noindent Из двух оценок лучшей считается та, чья матрица ковариаций меньше\footnote{Заметим, что в данном случае две оценки могут быть несравнимы}.

\noindent Перейдем к обобщению неравенства Крамера--Рао на многомерный случай. 

\noindent Введем оператор частого дифференциирования по $\theta$\footnote{Обратим внимание на то, что это именно строка}
$$ \frac{\partial}{\partial\theta} = \left(\frac{\partial}{\partial\theta_1},\ldots,\frac{\partial}{\partial\theta_r} \right)$$
И рассмотрим матрицу информации
$$ I(\theta) = E_\theta \left[\frac{\partial}{\partial\theta}\ln{p(X, \theta)}\right]^T \left[\frac{\partial}{\partial\theta}\ln{p(X, \theta)}\right]$$

Матрица $I(\theta)$ неотрицательно определена, обозначим это через $I(\theta)\geq 0$. Предположим, что $\forall\theta\in\Theta\,\,\exists I^{-1}(\theta)$. Рассмотрим матрицу частных производных размера $d\times r$

$$
\frac{\partial\tau}{\partial\theta} = \begin{pmatrix}
\frac{\partial\tau_1}{\partial\theta_1} & \frac{\partial\tau_1}{\partial\theta_2} &\ldots & \frac{\partial\tau_1}{\partial\theta_r} \\
\frac{\partial\tau_2}{\partial\theta_1} & \frac{\partial\tau_2}{\partial\theta_2} &\ldots & \frac{\partial\tau_2}{\partial\theta_r} \\       
\vdots & \vdots &\ddots & \vdots \\
\frac{\partial\tau_d}{\partial\theta_1} & \frac{\partial\tau_d}{\partial\theta_2} &\ldots & \frac{\partial\tau_d}{\partial\theta_r} \\ 
\end{pmatrix} 
$$

\begin{theorem}[\bf Обобщенное неравенство Крамера-Рао]
При принятых в одномерном случае условиях регулярности (с учетом нового смысла оператора $\frac{\partial}{\partial\theta})$
выполнено неравенство 
$$ E_\theta[\varphi(X) - \tau(\theta)][\varphi(X) - \tau(\theta)]^T \geq \frac{\partial\tau}{\partial\theta} I^{-1}(\theta) \frac{\partial\tau}{\partial\theta}^T $$
\end{theorem}
\begin{proof}
Не приводится.
\end{proof}
\noindent Обозначим через $C_{RM}$ класс несмещенных оценок с конечной ковариационной матрицей. 
\begin{mydef}
Эффективная в классе $C_{RM}$ оценка --- оценка, для которой в неравенстве Крамера-Рао достигается равенство. 
\end{mydef}
\begin{mydef}
Эффективная оценка называется оптимальной в классе $C_{RM}$.
\end{mydef}

\noindent Эффективная оценка $\varphi(X)$ существует тогда, и только тогда, когда параметрическая плотность представима в виде
$$ p(x, \theta) = \exp{\left\{ \varphi(x)^T A(\theta) + B(\theta) \right\}} \cdot h(x)$$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{6. Условное математическое ожидание (у.м.о.) относительно дискретной $\sigma$-алгебры – два эквивалентных определения.}

Пусть $(\Omega, \mathfrak{F}, \mathbb{P})$ --- вероятностное пространство, $\{A_1, A_2 \ldots \}$ --- разбиение $\Omega$ на события положительной вероятности. 
Далее, $U = \sigma(\{A_1, A_2 \ldots \})$ --- $\sigma$-алгебра, порожденная системой $\{A_i\}$. Очевидно, $U$ --- $\sigma$-подалгебра $\mathfrak{F}$.

\noindent Рассмотрим случайную величину $\xi:(\Omega, \mathfrak{F}) \rightarrow (\mathbb{R}, \mathfrak{B}(\mathbb{R}))$ с условием $E|\xi| < \infty$

\begin{mydef}
Условным математическим ожиданием величины $\xi$ относительно $\sigma$-алгебры $U$ называется случайная величина 
$$ \hat{\xi} = E(\xi|U) = \sum\limits_{k=1}^{\infty}{\frac{E(\xi, A_k)}{\mathbb{P}(A_k)}\mathbf{1}_{A_k}}, \qquad E(\xi, A_k) = \int\limits_{A_k}{\xi\cdot \mathbb{P}(d\omega)} = E(\xi\cdot\mathbf{1}_{A_k})$$
Таким образом, $U$ --- дискретная $\sigma$-алгебра, $\hat{\xi}$ --- дискретная случайная величина, принимающая на $A_k$ значение $\frac{E(\xi, A_k)}{\mathbb{P}(A_k)}$. 
\end{mydef}

\noindent Приведем без доказательства следующее:
\begin{lemma}
Пусть $U_\xi$ --- $\sigma$-алгебра, порожденная $\xi$ (То есть $U_\xi = \{\xi^{-1}(B):B\in\mathfrak{B}(\mathbb{R}) \}$. Тогда случайная величина $\eta$ $U_\xi$-измерима тогда и только тогда, когда $\eta = \varphi(\xi)$, $\varphi$--- борелевская функция 
\end{lemma}

\noindent Докажем, что $\hat{\xi}$ обладает следующими свойствами:
\begin{enumerate}
\item $\hat{\xi}$ измерима относительно $U$.
\item $\forall A\in U\quad E(\hat{\xi}|A) = E(\xi|A)$
\end{enumerate}

\begin{proof}
$ $
\begin{enumerate}
\item
$U$ порождается случайной величиной 
$$ \xi = \sum\limits_{k=1}^{\infty}{C_k \mathbf{1}_{A_k}}$$
Как было сказано, $\eta\quad U$-измерима $\Leftrightarrow$ $\eta = \varphi(\xi)$, где $\varphi$ --- борелевская функция. Таким образом,
$$ \eta = \sum\limits_{k=1}^{\infty}{\varphi(C_k) \mathbf{1}_{A_k}} $$
Из вида формулы для $\hat{\xi}$ теперь прямо следует, что она $U$-измерима
\item Из дискретности $U$ следует, что
$$ \forall A \in U \quad A = \sum\limits_{k=1}^{\infty}{A_{j_k}} $$
С учетом этого
$$ E(\hat{\xi}, A) = \sum\limits_{k=1}^{\infty}{E(\hat{\xi}, A_{j_k})} = \sum\limits_{k=1}^{\infty}{E(\hat{\xi}\cdot\mathbf{1}_{A_{j_k}})} = \sum\limits_{k=1}^{\infty}{E\left( \frac{E(\xi\cdot\mathbf{1}_{A_{j_k}})}{\mathbb{P}(A_{j_k})} \mathbf{1}_{A_{j_k}} \right)} = $$ 
$$ = \sum\limits_{k=1}^{\infty}{E(\xi\cdot\mathbf{1}_{A_{j_k}})} = E(\xi \cdot \mathbf{1}_A) = E(\xi, A) $$
\end{enumerate}
\end{proof}
\newpage
\begin{lemma}
Два описанных свойства эквивалентны определению УМО\footnote{Здесь и далее УМО - условное математическое ожидание}.
\end{lemma}
\begin{proof}
Нам осталось доказать, что из свойств следует определение.
$\hat{\xi}$ $U$-измерима, а значит
$$ \hat{\xi} = \sum\limits_{k=1}^{\infty}{\varphi(C_k) \mathbf{1}_{A_k}} $$
а также
$$ E(\hat{\xi}, A_j) = E(\xi, A_j) = E(C_j\cdot\mathbf{1}_{A_j}) = C_j\cdot\mathbb{P}(A_j) \Longrightarrow C_j = \frac{E(\xi, A_j)}{\mathbb{P}(A_j)} \Longrightarrow \hat{\xi} = \sum\limits_{k=1}^{\infty}{\frac{E(\xi, A_k)}{\mathbb{P}(A_k)}\mathbf{1}_{A_k}} $$
Таким образом, $\hat{\xi}$ --- УМО.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{7. Определение у.м.о. в общей ситуации (для скалярных и векторных случайных величин). Теорема существования.}

Обобщим понятие УМО на случай произвольной (не обязательно) дискретной $\sigma$-подалгебры событий.
\noindent Пусть $(\Omega, \mathfrak{F}, \mathbb{P})$ --- вероятностное пространство, $U$ --- произвольная $\sigma$-подалгебра $\mathfrak{F}$

\begin{mydef}
Условным математическим ожидание случайной величины $\xi$ (возможно, векторной) относительно $U$ называется случайная величина $\hat{\xi} = E(\xi|U)$, обладающая следующими свойствами:
\begin{enumerate}
\item $\hat{\xi} \quad U$-измерима
\item $\forall A\in U\quad E(\hat{\xi}, A) = E(\xi, A) \Longleftrightarrow \forall A\in U\quad \int\limits_A{\hat{\xi}\cdot \mathbb{P}(d\omega)} = \int\limits_A{\xi\cdot \mathbb{P}(d\omega)}$
\end{enumerate}
\end{mydef}

\noindent При дальнейших рассмотрениях нам понадобится следующая
\begin{theorem}[\bf Радон-Никодим]
Пусть $(X, \mathfrak{F}, \mu)$ --- пространство с $\sigma$-конечной мерой $\mu$. Тогда если мера $\nu:\mathfrak{F}\rightarrow\mathbb{R}$ абсолютно непрерывна относительно $\mu$ (то есть, $\forall A\in\mathfrak{F} \,\, \mu A = 0 \Rightarrow \nu A = 0)$. Тогда существует измеримая функция $f:X\rightarrow\mathbb{R}$, такая, что
$$ \forall A \in \mathfrak{F} \quad \nu(A) = \int\limits_A{f(x)\,\mu(dx)}$$
причем $f(\omega)$ единственна с точностью до задания на множестве меры нуль. Почти всюду 
$$ f(\omega) = \frac{d\nu}{d\mu}(\omega) \geq 0$$
\end{theorem}
\begin{proof}
Не приводится (вспоминаем действительный анализ :).
\end{proof}

\begin{theorem}[\bf Существования УМО]
Если $E|\xi| < \infty$, то $\hat{\xi}$ существует и единственно с точностью до задания на множестве меры 0.
\end{theorem}
\begin{proof}
$ $
\begin{enumerate}
\item Пусть $\xi$ --- неотрицательная случайная величина. Рассмотрим меру 
$$ Q(A) := \int\limits_A{\xi(\omega)\cdot\mathbb{P}(d\omega)}$$
Мера $Q$ абсолютна непрерывна относительно вероятности $\mathbb{P}$, следовательно, найдется такая $U$-измеримая величина $\hat{\xi}$, такая, что 
$$ \forall A\in U \quad Q(A) = \int\limits_A{\hat{\xi}(\omega)\cdot\mathbb{P}(d\omega)} = \int\limits_A{\xi(\omega)\cdot\mathbb{P}(d\omega)}$$
Таким образом, $\hat{\xi}(\omega) = \frac{dQ}{d\mathbb{P}}$ --- УМО.
\item Если $\xi$ --- произвольная случайная величина. Тогда $\xi$ допускает разложение в виде 
$$ \xi = \xi_+ - \xi_-,\qquad \xi_+ = \max{\{\xi, 0\}},\,\,\xi_- = \max{\{-\xi, 0\}} $$
Аналогично раскладывая $\hat{\xi} = \hat{\xi}_+ - \hat{\xi}_-$, мы можем очевидным образом обобщить результат пункта 1 на случай произвольной одномерной случайной величины с конечным первым моментом.
\item Рассмотрим теперь случайный вектор $\xi = (\xi_1,\ldots,\xi_s)^T$. Положим $\hat{\xi} = (\hat{\xi_1},\ldots,\hat{\xi_s})^T$. Измеримость $\hat{\xi}$ следует из покомпонентной измеримости и структуры $\mathfrak{B}(\mathbb{R}^n)$.
\item Единственность УМО. Доказывается от противного.
$$ \forall A \in U \quad \int\limits_A{\hat{\xi}\cdot\mathbb{P}(d\omega)} = \int\limits_A{\hat{\eta}\cdot\mathbb{P}(d\omega)} \Longrightarrow \forall i \,\, \mathbb{P}(\hat{\xi}_i = \hat{\eta}_i) = 1 \Longrightarrow \mathbb{P}(\hat{\xi} = \hat{\eta}) = 1 $$
\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{8. Основные свойства у.м.о (все с доказательством).}

Докажем некоторые свойства условных математических ожиданий. Как и ранее, $(\Omega, \mathfrak{F}, \mathbb{P})$ --- вероятностное пространство, $U$ --- произвольная $\sigma$-подалгебра $\mathfrak{F}$.

\noindent В пунктах 1-5 предполагается, что все случайные величины --- скалярные или векторные, $\xi_1 \leq \xi_2$, $\xi_n \nearrow \xi$ покомпонентно, $\forall n\in\mathbb{N}\,\,E|\xi_n| < \infty$, $E|\xi|<\infty$
\begin{enumerate}
\item 
\begin{enumerate}
\item $\forall c\in\mathbb{R} \,\, E(c\xi|U) = c(\xi|U)$
\item $E(\xi_1 + \xi_2|U) = E(\xi_1|U) + E(\xi_2|U)$
\item $\xi_1 \leq \xi_2 \text{ п.н.} \Longrightarrow E(\xi_1|U) \leq E(\xi_2|U)\text{ п.н.}$
\end{enumerate}
\begin{proof}
$ $
\begin{enumerate}
\item $c\hat{\xi}$ - $U$-измерима.
$$ \forall A \in U\quad E(c\hat{\xi}, A) = cE(\hat{\xi}, A) = cE(\xi,A) = E(c\xi, A)$$
\item $\hat{\xi}_1 + \hat{\xi_2}$ - $U$-измерима.
$$ \forall A \in U\quad E(\hat{\xi}_1 + \hat{\xi_2}, A) = E(\hat{\xi}_1, A) + E(\hat{\xi}_2, A) = E(\xi_1,A) + E(\xi_2,A)= E(\xi_1 + \xi_2, A)$$
\item $\hat{\xi}_i = E(\xi_i|U),\,\,i=1,2$

$$ \forall A \in U \quad E(\hat{\xi}_1, A) = \int\limits_A{\hat{\xi}_1(\omega)\cdot\mathbb{P}(d\omega)} = \int\limits_A{\xi_1(\omega)\cdot\mathbb{P}(d\omega)} \leq $$ 
$$ \leq \int\limits_A{\xi_2(\omega)\cdot\mathbb{P}(d\omega)} = \int\limits_A{\hat{\xi}_2(\omega)\cdot\mathbb{P}(d\omega)} = E(\hat{\xi}_2, A) $$
Таким образом,
$$ \forall A \in U \quad \int\limits_A{(\hat{\xi}_1 - \hat{\xi})(\omega)\cdot\mathbb{P}(d\omega)} \leq 0 \text{ п.н.} \Longrightarrow \hat{\xi}_1 - \hat{\xi}_2 \leq 0 \text{ п.н.}$$
\end{enumerate}
\end{proof}

\item Если $U$ и $\sigma(\xi) = U_\xi$ независимы, то $E(\xi|U) = E\xi = const \alprob$
\begin{proof}
Очевидно, $E\xi$ - $U$-измерима. Проверим, что
$$ \forall A \in U \quad E(E\xi, A) = E(\xi, A) $$
Действительно,
$$ E(\xi, A) = \int\limits_A{\xi(\omega)\cdot\mathbb{P}(d\omega)} = E(\xi \mathbf{1}_A) = E\xi\cdot\mathbb{P}(A) = E(E\xi, A)$$
\end{proof}

\item $ 0 \leq \xi_n \nearrow \xi \Longrightarrow E(\xi_n|U) \nearrow E(\xi|U) \alprob $
\begin{proof}
$\forall n \in \mathbb{N}\quad \hat{\xi}_{n+1} \geq \hat{\xi}_n$. По теореме о монотонной сходимости\footnote{Вспоминаем теорему Беппо Леви. Ограниченность интегралов в данном случае эквивалентна ограниченности первых моментов} найдется такая $U$-измеримая случайная величина $\hat{\xi}$, что
$$ \hat{\xi}_n \rightarrow \hat{\xi} \alprob $$
причем
$$ \forall A \in U \quad \integr{\hat{\xi}_n} \rightarrow \integr{\hat{\xi}} \Longrightarrow \integr{\xi_n} \rightarrow \integr{\xi} $$
а значит,
$$ \integr{\hat{\xi}} = \integr{\xi} $$ 
\end{proof}

\item Пусть $\eta:\Omega\rightarrow\mathbb{R}$ --- $U$-измеримая случайная величина, $E|\xi| < \infty$, $E|\xi\eta| < \infty$, тогда
$$ E(\xi\eta|U) = \eta E(\xi|U) \alprob $$

\begin{proof}
\begin{enumerate}

\item Пусть $\eta = \mathbf{1}_B,\,\,B\in U$. Тогда 
$$ \forall A \in U \quad \integr{E(\mathbf{1}_B\xi|U)} = \integr{\mathbf{1}_B\xi} = \int\limits_{AB}{\xi(\omega)\cdot\mathbb{P}(d\omega)} = \int\limits_{AB}{E(\xi|U)\cdot\mathbb{P}(d\omega)} = $$
$$ = \integr{\mathbf{1}_B E(\xi|U)} \Longrightarrow E(\mathbf{1}_B\xi|U) = \mathbf{1}_B E(\xi|U) $$
\item Из линейности УМО делаем вывод, что утверждение верно для простых функций, то есть функций вида 
$$ \eta = \sum\limits_{i=1}^k{C_i \cdot\mathbf{1}_{B_i}}$$
\item Случай $\xi \geq 0,\,\,\eta\geq 0$. Строим последовательность простых неотрицательных функций $\eta_n \nearrow \eta$. 
Тогда, по доказанному выше, с вероятностью 1 выполнено 
$$ E(\eta_n \xi|U) = \eta_n E(\xi|U) \nearrow \eta E(\xi|U) $$
С учетом этого факта
$$ E(\eta_n \xi|U) \nearrow E(\eta \xi | U) \alprob \Longrightarrow E(\eta\xi|U) = \eta E(\xi|U) \alprob $$
\item $\eta, \xi$ --- произвольные. Применяя разложения $\xi = \xi_+ - \xi_-$, $\eta = \eta_+ - \eta_-$ и пользуясь линейностью УМО, получаем требуемое утверждение.
\end{enumerate}
\end{proof}

\item \textbf{Формула полной вероятности}

$ E(E(\xi|U)) = E\xi $
\begin{proof}
$$ E(E(\xi|U)) = \int\limits_\Omega{E(\xi|U)\cdot\mathbb{P}(d\omega)} = \int\limits_\Omega{\xi(\omega)\cdot\mathbb{P}(d\omega)} = E\xi $$
\end{proof}

\item \textbf{Формула последовательного усреднения}

$U \subseteq U_1 \subseteq \mathfrak{F} $ --- цепочка $\sigma$-алгебр. Тогда $E(\xi|U) = E(E(\xi|U_1)|U)$
\begin{proof}
$$ \forall A (A\in U \Rightarrow A\in U_1) \Longrightarrow \integr{E(E(\xi|U_1)|U)} = \integr{E(\xi|U_1)} = \integr{\xi} = \integr{E(\xi|U)} $$
\end{proof}

\item Пусть $\xi:\Omega\rightarrow\mathbb{R}$ --- случайная величина с конечным вторым моментом ($\xi\in L_2(\Omega)$). 
Скалярное произведение в $L_2(\Omega)$ дается формулой $\langle \xi, \eta \rangle = E(\xi\eta)$. $H_U$ --- замкнутое в $L_2$ подпространство $U$-измеримых случайных величин. Тогда $E(\xi|U) = proj_{H_U}\xi$.

\noindent Таким образом, задача $E(\xi - a(\omega))^2 \rightarrow \min\limits_{a(\omega)\in H_U}$ имеет решение $a(\omega) = E(\xi|U)$
\begin{proof}
$$ E(\xi - E(\xi|U) + E(\xi|U) - a(\omega))^2 = E(\xi - E(\xi|U))^2 + 2E((\xi - E(\xi|U)(E(\xi|U) - a(\omega))) + E(E(\xi|U) - a(\omega))^2 $$
$$ E(E((\xi - E(\xi|U)(E(\xi|U) - a(\omega)))|U) = E((E(\xi|U) - a(\omega))(\xi - E(\xi|U)) = 0 \Longrightarrow $$
$$ E(\xi - a(\omega))^2 = E(\xi - E(\xi|U))^2 + E(E(\xi|U) - a(\omega))^2 $$
Таким образом, левая часть минимальна если, и только если $a(\omega) = E(\xi|U)$
\end{proof}

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{9. У.м.о., условная вероятность и условное (регулярное) распределение относительно случайных величин (векторов).}

$(\Omega, \mathfrak{F}, \mathbb{P})$ --- вероятностное пространство, $\xi\in\mathbb{R}^s,\,\,\eta\in\mathbb{R}^k$ --- случайные вектора. $U_\eta = \sigma(\eta)$ --- $\sigma$-алгебра, порожденная $\eta$. 

\begin{mydef}
Условным математическим ожиданием $\xi$ относительно $\eta$ называется величина 
$$ E(\xi|\eta) = E(\xi|U_\eta) $$
\end{mydef}

\noindent Поскольку $E(\xi|U_eta)$ --- $U_\eta$-измеримая функция получаем $E(\xi|U_eta) = \varphi(\eta),\,\,\varphi$ --- борелевская. $\varphi(y) = (\xi|\eta = y)$, и, кроме того,
$$ \forall B \in \mathfrak{B}(\mathbb{R}^k)\quad \int\limits_{\omega:\eta(\omega)\in B}{\varphi(\eta(\omega))\cdot\mathbb{P}(d\omega)} = \int\limits_{\omega:\eta(\omega)\in B}{\xi(\omega)\cdot\mathbb{P}(d\omega)} $$
Делаем замену $y = \eta(\omega)$, тогда $\varphi(y) = E(\xi|y)?$. Подставляя, получаем
$$ \int\limits_B{E(\xi|y)\cdot\mathbb{P}_\eta(dy)} = \int\limits_{\omega:\eta(\omega)\in B}{\xi(\omega)\cdot\mathbb{P}(d\omega)} $$
Следовательно, $E(\xi|y)$ --- борелевская функция, определенная с точностью до множества $\mathbb{P}_\eta$-меры 0.

\noindent Для дальнейших рассуждений заметим, что 
$$ \forall C \in \mathfrak{F} \quad \mathbb{P}(C) = E\mathbf{1}_C $$

\begin{mydef}
Пусть $C\in\mathfrak{F}$ --- некоторое событие. Условной вероятностью события относительно случайной величины $\eta$ называется величина
$$ \mathbb{P}(C|\eta) = E(\mathbf{1}_C|U_\eta)$$
\end{mydef}

В этом случае мы также делаем вывод о существовании борелевской функции $g_C(\eta) = \mathbb{P}(C|\eta)$. Тогда $g_C(y) = \mathbb{B}(C|\eta = y) = \mathbb{P}(C|y)$. $g_C$ удовлетворяет условию
$$ \forall B \in \mathfrak{B}(\mathbb{R}^k)\quad \int\limits_B{\mathbb{P}(C|y)\cdot\mathbb{P}_\eta(dy)} = \mathbb{P}(C, \eta\in B) $$

\begin{mydef}
Пусть $A\in\mathfrak{B}(\mathbb{R}^s),\,\,y\in\mathbb{R}^k$. Величина $\mathbb{P}(\xi\in A|y) = \mathbb{P}(\xi\in A|\eta = y)$ называется условным (регулярным) распределение $\xi$ при условии $\eta = y$, если выполнены следующии условия:
\begin{enumerate} 
\item При любом фиксированном $A$
$$ \mathbb{P}(\xi\in A|y) = \mathbb{P}\{\omega: \xi(\omega)\in A\} \text{ при условии } \eta = y $$
\item При каждом фиксированном $y$, за исключением, быть может, множества $\mathbb{P}_\eta$-меры 0, $\mathbb{P}(\xi\in A|y) $ --- распределение вероятностей на $A$.
\end{enumerate}
\end{mydef}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{10. Условная плотность вероятности, теорема о ее вычислении и о вычислении у.м.о. через условную плотность. Примеры.}

\begin{mydef}
Пусть $f(x|y)$ --- неотрицательная функция пары векторных аргументов $x\in\mathbb{R}^s,\,\,y\in\mathbb{R}^k$, измеримая по совокупности $(x, y)$. Потребуем, чтобы равенство
$$ \forall A \in \mathfrak{\mathbb{R}^s} \quad \mathbb{P}(\xi\in A|\eta = y) = \int\limits_A{f(x|y)\cdot\mathbb{P}(dx)}$$
выполнялось при всех $y$, за исключением, быть может, множества $\mathbb{P}_\eta$-меры 0.

\noindent В таком случае $f(x|y)$ называется условной плотностью $\xi$ при условии $\eta = y$.
\end{mydef}

\noindent Заметим, что в данном случае выполняется
$$ \forall A \in \mathfrak{\mathbb{R}^s} \,\,\,\, \forall B \in \mathfrak{\mathbb{R}^k} \quad
\int\limits_{y\in B}{\int\limits_{x\in A} { f(x|y)\,\mu(dx)\,\mathbb{P}_\eta(dy)} } = \mathbb{P}(\xi\in A, \eta\in B)$$

\begin{theorem}
Пусть $\mu$ --- мера на $\mathbb{R}^s$, $\lambda$ --- мера на $\mathbb{R}^k$.
Cовместное распределение величин $\xi$ и $\eta$ имеет плотность $f(x, y)$ относительно $\nu = \mu \otimes \lambda$.
Тогда
\begin{enumerate}
\item Плотность $\eta$ выражается как
$$ q(y) = \int\limits_{\mathbb{R}^s}{f(x, y)\,\mu(dx)}$$
\item Условная плотность выражается следующим образом:
\begin{equation*}
\begin{split}
f(x|y)
&=\left\{
        \begin{array}{ll}
          \frac{f(x, y)}{q(y)}, & q(y) \not= 0 \\
          0, & q(y) = 0
        \end{array}
      \right.
\end{split}
\end{equation*} 
\item Кроме того, для борелевской функции $\varphi$
$$ E(\varphi(\xi)|\eta = y) = \int\limits_{\mathbb{R}^s}{\varphi(\xi)\,f(x|y)\,\mu(dx)}$$
\end{enumerate}
\end{theorem}
\begin{proof}
Функция $f(x|y)$, очевидно, неотрицательна и измерима. Далее, 
$$ \forall A \in \mathfrak{\mathbb{R}^s} \,\,\,\, \forall B \in \mathfrak{\mathbb{R}^k} \quad
\int\limits_{y\in B,\, q(y) \not= 0}{\int\limits_{x\in A} { \frac{f(x,y)}{q(y)}q(y)\,\mu(dx)\,\lambda(dy) } } = \iint\limits_{A\times B}{f(x,y)\,\mu(dx)\,\mathbb{P}_\eta(dy)} = P(\xi\in A, \eta\in B) $$

Докажем теперь все утверждения теоремы.
\begin{enumerate}
\item Плотность $\eta$
$$ \forall B \in \mathfrak{\mathbb{R}^k} \quad \mathbb{P}(\eta\in B) = \mathbb{P}(\eta\in B, \xi\in\mathbb{R}^s) = \iint\limits_{\mathbb{R}^s\times B}{f(x,y)\,\mu(dx)\,\lambda(dy)} = $$ 
$$ = \int\limits_B{\left( \int\limits_{\mathbb{R}^s}{f(x, y)\,\mu(dx)}  \right)\,\lambda(dy)} = \int\limits_B{q(y)\lambda(dy)}$$
Таким образом, $q(y)$ --- действительно плотность $\eta$.
\item Для почти всех $y$
$$ \int\limits_{\mathbb{R}^s}{f(x|y)\,\mu(dx)} = 1$$
\item Нужно проверить равенство 
$$ E(\varphi(\xi)|\eta = y) = \int\limits_{\mathbb{R}^s}{\varphi(\xi)\,f(x|y)\,\mu(dx)}$$
Действительно,
$$ \forall B \in \mathfrak{\mathbb{R}^k} \quad \int\limits_B {\left( \int\limits_{\mathbb{R}^s}{\varphi(x)f(x|y)\,\mu(dx)} \right)\,\mathbb{P}_\eta(dy)} = \int\limits_B { \int\limits_{\mathbb{R}^s}{\varphi(x)\frac{f(x, y)}{q(y)}q(y)\,\mu(dx)} \,\lambda(dy)} = $$
$$ = \int\limits_B { \int\limits_{\mathbb{R}^s}{\varphi(x)f(x, y)\,\mu(dx)} \,\lambda(dy)} = E(\varphi(\xi)\cdot\mathbf{1}_{\eta\in B}) = \int\limits_{\eta\in B}{\varphi(\xi)\,\mathbb{P}(d\omega)}$$ 
\end{enumerate}
\end{proof}

Рассмотрим теперь важный частный случай ситуации, описанной в теореме.

Пусть величины $\xi$ и $\eta$ дискретны, $x_i\in\mathbb{R}^s$, $y_j\in\mathbb{R}^s$ --- принимаемые ими значения. $f(x_i, y_j) = \mathbb{P}(\xi = x_i, \eta = y_j)$. Плотность $\eta$ задается как $q(y) = \mathbb{P}(\eta = y)$. Условная плотность:
$$ f(x_i|y_j) = \frac{\mathbb{P}(\xi = x_i, \eta = y_j)}{\mathbb{P}(\eta = y_j)} = \mathbb{P}(\xi = x_i|\eta = y_j)$$
Очевидно, $f(x|y) = 0$ при $y\not\in \{y_j\} = Y$
\begin{equation*}
\begin{split}
\mathbb{P}(\xi\in A|\eta = y) = \int\limits_A{f(x|y)\,\mu(dx)} = \sum\limits_{x_j\in A}{f(x_i|y)}
&=\left\{
        \begin{array}{ll}
          \sum\limits_{x_j\in A}{\mathbb{P}(\xi = x_i|\eta = y)}, & y\in Y \\
          0, & y \not\in Y
        \end{array}
      \right.
\end{split}
\end{equation*} 
В случае независимости $\xi$ и $\theta$
$$ \sum\limits_{x_j\in A}{\mathbb{P}(\xi = x_i|\eta = y)} = \sum\limits_{x_j\in A}{\mathbb{P}(\xi = x_i)} $$
Условное матожидание:
\begin{equation*}
\begin{split}
E(\xi|y) = \int\limits_{\mathbb{R}^s}{xf(x|y)\,\mu(dx)}
&=\left\{
        \begin{array}{ll}
          \sum\limits_{x_j\in A}{x_i \mathbb{P}(\xi = x_i|\eta = y)}, & y\in Y \\
          0, & y \not\in Y
        \end{array}
      \right.
\end{split}
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{11. Достаточные статистики, теорема факторизации (доказательства для дискретного и гладкого случаев). Примеры.}

Пусть имеется выборка $X = (X_1,\ldots,X_n)$, распределение компонент которой принадлежит некоторому семейству $\mathbb{P}_X \in \{ \mathbb{P}_\theta: \theta \in \Theta \}$.

\begin{mydef}
Статистика $T(X)$ называется достаточной для параметра $\theta$, если найдется вариант условного распределния $\mathbb{P}(X\in A|T(X) = t)$, не зависящий от $\theta$ ни при одном значении $t$.
(Таким образом, вся информация о $\theta$ содержится в статистике $T(X)$.
\end{mydef}

Пусть $I^T(\theta)$ --- информация Фишера о $\theta$, содержащаяся в $T(X)$, Пусть $I^X(\theta)$ --- информация Фишера о $\theta$, содержащаяся в $X$. Верно, что $\forall \theta I^T(\theta) \leq I^X(\theta)$. Статистика $T(X)$ является достаточной в том и только в том случае, если $I^T(\theta) = I^X(\theta)$.

Заметим, что если $\varphi$ биективна и измерима вместе со своей обратной, то если $T$ --- достаточная, то таковой же является и $\varphi(T)$.

\begin{theorem}[\bf Критерий факторизации Неймана-Фишера]
Пусть наблюдение $X$ имеет плотность $p(x, \theta)$ относительно меры $\mu$ в $(\mathbb{R}^n, \mathfrak{B}(\mathbb{R}^n))$. Тогда статистика $T(X)$ достаточная в том, и только в том случае, если:
$$ p(x, \theta) = \psi(T(X), \theta)h(x), $$
причем $\psi(s, \theta)\geq 0$ измерима относительно $s$, $h(x)\geq 0$ измерима относительно $x$.
\end{theorem}
\begin{proof}
$ $
\begin{enumerate}
\item Докажем сперва для дискретного случая.

Пусть $X$ --- дискретный вектор, $\mathbf{X}$ --- множество возможных значений, $\mathbf{X}_T$ --- множество значений статистики $T$. Тогда 
\begin{equation*}
\begin{split}
P(X\in A| T(X) = t)
&=\left\{
        \begin{array}{ll}
          \sum\limits_{x_i\in A}{\mathbb{P}_\theta(X = x_i|T(X) = t)}, & t\in \mathbf{X}_T \\
          0, & t \not\in \mathbf{X}_T
        \end{array}
      \right.
\end{split}
\end{equation*}
Таким образом, условное распределение не зависит от $\theta$, то есть 
$$\forall t \in \mathbf{X}_T\,\,\forall x\in\mathbf{X}\quad \mathbb{P}_\theta(X = x|T(X) = t) \text{ не зависит от } \theta $$
Явная формула:
\begin{equation*}
\begin{split}
\mathbb{P}_\theta(X = x|T(X) = t) = \frac{\mathbb{P}_\theta(X = x, T(X) = t)}{\mathbb{P}_\theta(T(X) = t)}
&=\left\{
        \begin{array}{ll}
          \frac{\mathbb{P}_\theta(X = x)}{\mathbb{P}_\theta(T(X) = t)}, & T(X) = t \\
          0, & T(X) \not= t
        \end{array}
      \right.
\end{split}
\end{equation*}
То есть условное распределение не зависит от $\theta$, что эквивалентно тому, что
$$ \forall x \in \mathbf{X} \quad A = \frac{\mathbb{P}_\theta(X = x)}{\mathbb{P}_\theta(T(X) = T(x))} $$
Если $\mathbb{P}_\theta(X = x) = p(x, \theta) = \psi(T(X), \theta)h(x)$, то
$$ \frac{\mathbb{P}_\theta(X = x)}{\mathbb{P}_\theta(T(X) = T(x))} = \frac{\psi(T(X), \theta)h(x)}{\sum\limits_{y:T(y)=T(x)}{p(y, \theta)}} = \frac{\psi(T(X), \theta)h(x)}{\sum\limits_{y:T(y)=T(x)}{\psi(T(y), \theta)h(y)}} = \frac{h(x)}{\sum\limits_{y:T(y)=T(x)}{h(y)}}$$
Полученный результат не зависит от $\theta$, значит $T(X)$ --- достаточная статистика.

\noindent Обратно, пусть 
$$ \forall x \in \mathbf{X} \quad \frac{\mathbb{P}_\theta(X = x)}{\mathbb{P}_\theta(T(X) = T(x))} = h(x) \text{ не зависит от }\theta $$
В таком случае
$$ \mathbb{P}_\theta(X = x) = h(x)\cdot\mathbb{P}_\theta(T(X) = T(x)) = h(x)\cdot\sum\limits_{y:T(y)=T(x)}{p(y, \theta)} = h(x)\cdot\psi(T(x), \theta) $$
\item Рассмотрим теперь гладкий случай
Через $\mu$ обозначим стандартную меру Лебега. Потребуем,, чтобы $N_p = \{x: p(x, \theta) > 0\}$ не зависело $\theta$
В таком случае существует замена $y_1 = T(X), y_2 = y_2(x),\ldots,y_n=y_n(x)$, разрешимая относительно $x_i:x_i = x_i(y_1,\ldots,y_n)$, такая, что 
$$ J(y) = \det{\left( \frac{\partial x_i(y)}{\partial y_j}\right)} \not= 0$$
Тогда плотность вектора $Y = (y_1,\ldots,y_n)^T$ выражается по обычной формуле замены переменных:
$$ g(y, \theta) = p(x(y), \theta) |J(y)|$$
Плотность отдельной компоненты:
$$ g^{(1)}(y_1, \theta) = \idotsint\limits_{\mathbb{R}^{n-1}}{p(x(y), \theta) |J(y)|\,dy_2\ldots dy_n}$$
С точки зрения событий
$$ \forall A \in \mathfrak{B}(\mathbb{R}^n)\quad (X\in A)\Leftrightarrow(X(Y) \in A)\Leftrightarrow(Y\in X^{-1}(A))$$
Таким образом, условное распределение $X$ не зависит от $\theta$ если, и только если условное распределение $Y$ не зависит от $\theta$. $(t,y_2,\ldots,y_n)$ не зависит от $\theta$ $\Leftrightarrow$ $(y_2,\ldots,y_n)$ не зависит от $\theta$. Вычислим условную плотность вектора $(y_2,\ldots,y_n)^T$ (обозначим ее через $\varphi_\theta(y|t)$).
\begin{equation*}
\begin{split}
\varphi_\theta(y|t)
&=\left\{
        \begin{array}{ll}
          \left.\frac{g(y, \theta)}{g^{(1)}(t, \theta)}\right|_{y_1 = t}, & g^{(1)}(t, \theta) \not= 0 \\
          0, & g^{(1)}(t, \theta)=0
        \end{array}
      \right. 
\quad\Longleftrightarrow \quad\varphi_\theta(y|t)=\left\{
        \begin{array}{ll}
          \left.\frac{p(x(y), \theta) |J(y)|}{g^{(1)}(t, \theta)}\right|_{y_1 = t}, & g^{(1)}(t, \theta) \not= 0 \\
          0, & g^{(1)}(t, \theta)=0
        \end{array}
      \right.
\end{split}
\end{equation*}

Нам нужно показать, что 
$$ p(x, \theta) = \psi(T(X), \theta)h(x) \Longleftrightarrow \varphi_\theta(y|t) = \left.\frac{p(x(y), \theta) |J(y)|}{g^{(1)}(t, \theta)}\right|_{y_1 = t} \text{  не зависит от } \theta$$
\begin{enumerate}
\item $\Rightarrow$

$$ \varphi_\theta(y|t) = \left.\frac{\psi(t, \theta)\cdot h(x(y)) \cdot |J(y)|}{\idotsint\limits_{\mathbb{R}^{n-1}}{\psi(t, \theta)\cdot h(x(y))\cdot |J(y)|\,dy_2\ldots dy_n}}\right|_{y_1 = t} \text{ не зависит от } \theta$$

\item $\Leftarrow$

Пусть $\varphi_\theta(y|t)$ не зависит от $\theta$. Заменим $t$ на $y_1$. Тогда, положив $g^{(1)}(y_1, \theta) = \psi(y_1, \theta)$, $\frac{\varphi_\theta(y|y_1)}{|J(y)|} = h(y)$, получим
$$ p(x(y), \theta) = g^{(1)}(y_1, \theta) \cdot \frac{\varphi_\theta(y|y_1)}{|J(y)|} = \psi(T(X), \theta)\cdot h(x)$$ 
\end{enumerate}
\end{enumerate}
\end{proof}

Приведем примеры нахождения достаточных статистик. 
\begin{enumerate}
\item 
Пусть имеется выборка $X=(X_1,\ldots,X_n)$ объема $n$ из пуассоновского распределения с показателем $\theta>0:X_1\sim Pois(\theta)$. Положим $x=(x_1,\ldots,x_n), x_i\in\mathbb{N}_0$. Совместная плотность а таком случае выражается в виде 
$$ p(x, \theta) = \mathbb{P}_\theta(X = x) = \prod\limits_{i=1}^{n}{\mathbb{P}_\theta(X_i = x_i)} =  \prod\limits_{i=1}^{n}{\frac{\theta^{x_i}}{x_i!}e^{-\theta}} = e^{-n\theta}\cdot\frac{\theta^{\sum_{i=1}^n{x_i}}}{\prod\limits_{i=1}^n{x_i!}} = \psi\left(\sum\limits_{i=1}^n{x_i}, \theta\right)\cdot h(x) $$
Следовательно, $T(X) = \sum\limits_{i=1}^n{X_i}$ --- достаточная статистика. $T(X) \sim Pois(n\theta)$. Вычислим для нее и для исходной выборки информацию Фишера. Обозначим через $f(x, \theta)$ плотность распределение отдельной компоненты. По условию, $f(x, \theta) = \frac{\theta^x}{x!}e^{-\theta}$.
$$ i(\theta) = E_\theta\left( \frac{\partial}{\partial\theta} \ln{f(X_1, \theta)}\right)^2 = E_\theta\left(\frac{\partial}{\partial\theta} (X_1\ln{\theta} - \theta - \ln{(x_1!)}) \right)^2 = $$
$$ = E_\theta\left(\frac{X_1}{\theta} - 1\right)^2 = \frac{1}{\theta^2}E_\theta(X_1 - \theta)^2 =\footnote{Для случайной величины $\xi \sim Pois(\theta) \quad E\xi = D\xi = \theta$} = \frac{1}{\theta^2}D_\theta X_1 = \frac{1}{\theta}$$
Полная информация в таком случае равна
$$ I^X(\theta) = n\cdot i(\theta) = \frac{n}{\theta} $$
Информация достаточной статистики
$$ I^T(\theta) = E_\theta\left( \frac{\partial}{\partial\theta} (T\ln{(n\theta)} - n\theta - \ln{(T!)}) \right)^2 = E_\theta\left(\frac{T}{\theta} - n\right)^2 = \frac{1}{\theta^2}E_\theta(T - n\theta)^2 = \frac{1}{\theta^2}DT = \frac{n}{\theta} = I^X(\theta)$$
Полученное равенство подтверждает достаточность статистики $T$

\item Пусть $X = (X_1,\ldots,X_N)$ --- выборка из нормального распределения $\mathcal{N}(\theta_1, \theta_2^2)$. $\theta = (\theta_1, \theta_2^2)$. Совместная плотность
$$ p(x, \theta) = \prod\limits_{i=1}^n{\left( \frac{1}{\sqrt{2\pi}\theta_2}\exp{\left\{ -\frac{1}{2\theta_2^2} (x_i - \theta_1)^2 \right\}}\right)} = \left(\frac{1}{\sqrt{2\pi}\theta_2}\right)^n \exp{\left\{ -\frac{1}{2\theta_2^2} \sum\limits_{i=1}^n{(x_i - \theta_1)^2} \right\}} = $$
$$ = \left(\frac{1}{\sqrt{2\pi}\theta_2}\right)^n \exp{\left\{ -\frac{1}{2\theta_2^2} \left( \sum\limits_{i=1}^n{x_i^2} - 2\theta_1\sum\limits_{i=1}^n{x_i} + n\theta_1^2 \right)\right\}} $$
Таким образом, достаточная статистика 
$$ T = (T_1, T_2) = \left(\sum\limits_{i=1}^n{x_i}, \sum\limits_{i=1}^n{x_i^2} \right) $$
После допустимого преобразования получаем достаточную статистику, являющуюся несмещенной оценкой параметра $\theta = (\theta_1, \theta_2^2)$:
$$ \tilde{T} = \left(\frac{1}{n}\sum\limits_{i=1}^n{x_i}, \frac{1}{n-1}\sum\limits_{i=1}^n{x_i^2} \right) = (\overline{X}, S^2) $$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{12. Теорема об улучшении несмещенной оценки усреднением по достаточной статистике (теорема Рао – Блекуэлла -- Колмогорова).}

\begin{theorem}[\bf Рао - Блекуэлл - Колмогоров]
Пусть $X=(X_1,\ldots,X_n)$ --- выборка, $T = T(X)$ --- достаточная статистика, $\tilde{\tau}_n$ --- несмещенная оценка $\tau(\theta)\in\mathbb{R}^m$ \\
Положим $\tau_n^* = E_\theta(\tilde{\tau}_n|T)$. Тогда $\tau_n^*$ обладает следующими свойствами:
\begin{enumerate}
\item $\tau_n^*$ зависит от $X$ только через $T(X)$.
\item $\tau_n^*$ --- несмещенная оценка $\tau(\theta)$ с конечной ковариационной матрицей.
\item $\forall \theta\in\Theta D_\theta\tau_n^* \leq D_\theta\tilde{\tau}_n$, причем равенство достигается если, и только если $\tilde{\tau}_n = \tau_n^*$ с $\mathbb{P}_\theta$-вероятностью 1.
\end{enumerate}
\begin{proof}
$ $
\begin{enumerate}
\item $\tau_n^*$ --- несмещенная, так как
$$ E_\theta\tau_n^* = E_\theta(E_\theta(\tilde{\tau}_n|T(X))) = E_\theta\tilde{\tau}_n = \tau(\theta)$$
Далее в доказательстве будет использовано неравенство Йенсена\footnote{Пусть $g(x)$ --- выпуклая функция, $\xi$ --- некоторая случайная величина, $U$ --- $\sigma$-подалгебра событий. Тогда $g(E(\xi|U)) \leq E(g(\xi)|U) $}
Положим $g(x) = x^2$. Тогда 
$$ \left(E(\xi|U)\right)^2 \leq E(\xi^2|U) \alprob \Longrightarrow E(E(\xi|U))^2 \leq E\xi^2 < \infty $$
Таким образом, из конечности ковариационной матрицы $\tilde{\tau}_n$ следует ее конечность для $\tau_n^*$
\item $T(X)$ --- достаточная статистика, а значит, борелевская функция $m(T) = E_\theta(\tilde{\tau}_n|T)$ не зависит от $\theta$, что и влечет за собой нужный факт.
\item Нам необходимо доказать, что\footnote{Заметим, что в данном случае $\alpha^T$ --- это просто транспонированный вектор и $T$ не имеет отношения к фигурирующей в теореме достаточной статистике}
$$ \forall\theta\in\Theta\,\,\forall\alpha\in\mathbb{R}^m\quad \alpha^T D_\theta\tau_n^*\alpha \leq \alpha^T D_\theta\tilde{\tau}_n\alpha $$
$$ \alpha^T D_\theta\tau_n^*\alpha = E_\theta(\alpha^T\tau_n^* - \alpha^T\tau(\theta))^2 = D_\theta(\alpha^T\tau_n^*) $$
Таким образом, нужное нам неравенство можно переписать в виде 
$$ D_\theta(\alpha^T\tau_n^*) \leq D_\theta(\alpha^T\tilde{\tau}_n) $$
Вычислим $D_\theta(\alpha^T\tilde{\tau}_n)$
$$ D_\theta(\alpha^T\tilde{\tau}_n) = E_\theta(\alpha^T\tilde{\tau}_n - \alpha^T\tau(\theta))^2 = E_\theta(\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^* + \alpha^T\tau_n^* - \alpha^T\tau(\theta))^2 = E_\theta(\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)^2\,\, + $$
$$ + E_\theta(\alpha^T\tau_n^* - \alpha^T\tau(\theta))^2 + 2E_\theta((\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)(\alpha^T\tau_n^* - \alpha^T\tau(\theta))) $$
Из $T$-измеримости $\alpha^T\tau_n^* - \alpha^T\tau(\theta)$ следует, что 
$$ E_\theta((\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)(\alpha^T\tau_n^* - \alpha^T\tau(\theta))) = E_\theta(E_\theta((\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)(\alpha^T\tau_n^* - \alpha^T\tau(\theta)))|T) = $$
$$ = E_\theta((\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)E_\theta(\alpha^T\tau_n^* - \alpha^T\tau(\theta)) |T) = 0 \Longrightarrow $$
$$ \Longrightarrow D_\theta(\alpha^T\tilde{\tau}_n) = E_\theta(\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)^2 + E_\theta(\alpha^T\tau_n^* - \alpha^T\tau(\theta))^2 \geq D_\theta(\alpha^T\tau_n^*) = E_\theta(\alpha^T\tau_n^* - \alpha^T\tau(\theta))^2 $$
Равенство достигается если, и только если 
$$ E_\theta(\alpha^T\tilde{\tau}_n - \alpha^T\tau_n^*)^2 = 0 \Longleftrightarrow \alpha^T\tilde{\tau}_n - \alpha^T\tau_n^* = 0 \alprob \Longleftrightarrow \tilde{\tau}_n = \tau_n^* \alprob $$
\end{enumerate}
\end{proof}
\end{theorem}

\noindent В конце приведем важное утверждение, еще раз доказывающее полезность достаточных статистик
\begin{lemma}
Для любой функции $\tau(\theta)$ оптимальньная оценка, если она существует, имеет вид $g(T)$, где $T$ --- достаточная статистика
\end{lemma}
\begin{proof}
Если $\tilde{\tau}_n$ --- оптимальная, то таковой же является $\tau_n^* = E_\theta(\tilde{\tau}_n|T)$. Из равенства их дисперсий следует, что они почти всюду совпадают. При этом $\tau_n^*$ является функцией $T$.
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{13 .Полные статистики. Оптимальное оценивание при наличии полной достаточной статистики (теорема Лемана –  Шефаре).Примеры для гауссовских, пуассоновских, биномиальных, равномерных выборок.}

Рассмотрим статистику $T=T(X)\in\mathbb{R}^l,\,\,l\geq k,\,\,k$ --- размерность вектора параметров.
\begin{mydef}
$T$ --- полная статистика, если 
$$ \forall\theta\in\Theta\quad E_\theta\varphi(T(X)) = 0 \Longrightarrow \mathbb{P}_\theta(\varphi(T(X)) = 0) = 1$$
\end{mydef}

Пусть $T(X)$ имеет плотность $q(x, \theta)$. Тогда 
$$ \mathbb{P}_\theta(\varphi(T(X)) = 0) = 1 \Longleftrightarrow 0 = \mathbb{P}_\theta(\varphi(T(X)) \not= 0) = E_\theta\mathbf{1}_{\varphi(T(X)) \not= 0} = \int\limits_{N_q}{\mathbf{1}_{\varphi(s) \not= 0}\,q(s,\theta)\,\mu(ds)}$$
Следовательно, $\mathbf{1}_{\varphi(s) \not= 0} = 0$ почти всюду по $\mu$ на $N_q$ $\Longleftrightarrow \mathbf{1}_{\varphi(s) = 0} = 1$.
Рассмотрим 2 частных случая, в которых последнее равенство выполнено:
\begin{enumerate}
\item $T(X)$ дискретна, $\mathcal{X}_T^\theta$ --- ее множество значений. Тогда $T(X)$ полна если, и только если 
$$ \forall\theta\in\Theta\,\,\forall s\in\mathcal{X}_T^\theta\quad \varphi(s) = 0 $$
\item $T(X)$ абсолютно непрерывна. Тогда $\varphi(s) = 0$ почти всюду по мере Лебега на $N_q$

\begin{theorem}
Пусть $T=T(X)$ --- полная достаточная статистика для $\theta$, $g:\mathbb{R}^l\rightarrow\mathbb{R}^k$ --- борелевская функция и $g(T)$ имеет конечную ковариационную матрицу. В таком случае $g(T)$ является оптимальной оценкой свлего матожидания $\tau(\theta) = E_\theta g(T)$
\end{theorem}
\begin{proof}
Оптимальная оценка для $\tau(\theta)$ по доказанному ранее имеет вид $\psi(T)$. Из несмещенности получаем $E_\theta(g(T) - \psi(T)) = 0$. Из достаточности $T$ следует, что $g(T)=\psi(T)$
Таким образом, оптимальная оценка единственна с точностью до почти наверное и является функцией достаточной статистики.
\end{proof}

Полные достаточные статистики для выборок из классических распределений\footnote{Все элементы выборок полагаются распределенными одинаково и независимыми в совокупности}:
\begin{enumerate}
\item $X = (X_1,\ldots,X_n)$, $X_i \sim Pois(\theta), \theta > 0$ Достаточная статистика $T(X) = \sum\limits_{i=1}^n{X_i}$
\item $X = (X_1,\ldots,X_n)$, $X_i$ --- бернуллиевские с показателем $p$. Достаточная статистика $T(X) = \sum\limits_{i=1}^n{X_i}$
\item $X = (X_1,\ldots,X_n)$, $X_i \sim U(a, b)\footnote{Равномерное на отрезке $[a,b]$ распределение}$ Достаточная статистика $T(X) = \left(\min\limits_{1 \leq i \leq n}X_i,\max\limits_{1 \leq i \leq n}X_i\right)$
\item $X = (X_1,\ldots,X_n)$, $X_i \sim \mathcal{N}(\mu, \sigma^2)$ Достаточная статистика $T(X) = \left(\sum\limits_{i=1}^nX_i, \sum\limits_{i=1}^nX_i^2\right)$
\end{enumerate} 

\end{enumerate} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{14. Определение и основные свойства многомерного гауссовского закона (все с доказательством).}

Напомним
\begin{mydef}
Характеристической функцией случайной величины $\xi$ называется функция 
$$ \varphi_\xi(t) = Ee^{it\xi} $$
В многомерном случае под $t\xi$ понимается скалярное произведение
\end{mydef}

\begin{mydef}
Вектор $\xi = (\xi_1,\ldots.\xi_n)^T$ имеет $n$-мерное гауссовское (нормальное) распределение, если его характеристическая функция имеет вид
$$ \varphi_\xi(t) = \expon{it^Ta - \frac{1}{2}t^TKt} $$
$ t = (t_1,\ldots,t_n)^T\in\mathbb{R}^n,\,\,a = (a_1,\ldots,a_n)^T\in\mathbb{R}^n$ --- постоянный вектор, $K\in M_n(\mathbb{R})$ --- постоянная положительно определенная симметрическая матрица.\\
Обозначение: $\xi\sim\mathcal{N}(a, K)$
\end{mydef}

Основные свойства многомерного нормального распределения:
\begin{enumerate}
\item $\xi_i \sim \mathcal{N}(a_i, K_{ii}$
\item $K_{ij} = cov(\xi_i,\xi_j),\quad K_{ii} = D\xi_i$
\item $\xi_i$ независимы если, и только если $ K$ диагональна.
\item $\xi\sim\mathcal{N}(a, K), A\in M_{m\times n}(\mathbb{R}), b\in\mathbb{R}^m \Longrightarrow \eta = A\xi+b\sim\mathcal{N}(Aa + b, AKA^T)$
\item $\xi\sim\mathcal{N}(a, K), K > 0 \Longleftrightarrow \xi$ имеет плотность
$$ p_\xi(x) = \frac{1}{(2\pi)^{\frac{n}{2}} (\det{K})^{\frac{1}{2}}}\exp{ \left\{ -\frac{1}{2} (x-a)^T K^{-1}(x-a) \right\} }$$
\end{enumerate}

\begin{proof}
$ $
\begin{enumerate}
\item $\varphi_{\xi_i}(t) = \varphi_\xi(0,\ldots i \ldots, 0) = \expon{it_i a_i - \frac{1}{2}t_i^2K_{ii}}$
\item Положим $\eta = \xi - a$. Тогда 
$$ cov(\xi_i, \xi_j) = E\eta_i\eta_j = \left.\frac{\partial^2 \varphi_\xi}{\partial t_i \partial t_j}\right|_0 = K_{ii}$$
\item Надо доказать только, что из диагональности $K$ следует независимость $\xi_i$
$$ \forall i \not= j\quad K_{ij} \Longrightarrow \varphi_\xi(t) = \expon{it^Ta - \frac{1}{2}\sum\limits_{i=1}^n{K_{ii}t_i^2}} = \prod\limits_{i=1}^n{\expon{it_i a_i - \frac{1}{2} K_{ii}t_i^2}} = \prod\limits_{i=1}^n{\varphi_{\xi_i}(t_i)}$$
Таким образом, $\xi_i$ независимы.
\item 
$$ \varphi_\eta(s) = E\expon{it^T(A\xi + b)} = \expon{is^tb}\cdot E\expon{i(A^Ts)^T\xi} = \expon{is^T(Aa + b) -\frac{1}{2}s^T(AKA^T)s}$$
\item Введем некоторые обозначения. Пусть $K$ --- симметрическая неотрицательно определенная матрица. Тогда 
$$ \exists C \in O_n(\mathbb{R}) \quad CKC^T = diag(d_1,\ldots,d_n),\,\,d_i > 0 $$
Определим матрицу $\sqrt{K} = K^\frac{1}{2} := C^TKC$. Она обладает следующими свойствами:
\begin{enumerate}
\item $K^\frac{1}{2} = (K^\frac{1}{2})^T$
\item $K^\frac{1}{2} \times K^\frac{1}{2} = K$
\item $K > 0 \Rightarrow K^{-\frac{1}{2}}KK^{-\frac{1}{2}} = E_n$
\item $\det{K^\frac{1}{2}} = (\det{K})^\frac{1}{2}$
\end{enumerate}

Приведем также без доказательства нужные факты о векторных случайных величинах. Пусть $Z, Y$ --- случайные векторы размерности $n$, причем $Z = AY + a$, где $A\in GL_n(\mathbb{R},\,\,b\in\mathbb{R}^n$. Тогда
$$ p_Z(x) = \frac{1}{|\det{A}|}\cdot p_Y(A^{-1}(x - b)) $$
Вернемся к доказательству. Положим $\eta = K^{-\frac{1}{2}}(\xi - a)$. $E\eta = 0,\,\,E\eta\eta^T = E_n\in M_n(\mathbb{R})$. С учетом того, что $\eta$ --- гауссовский вектор, мы заключаем, что $\eta \sim \mathcal{N}(0, E_n)$. В таком случае, $\xi = K^\frac{1}{2} + a$. Компоненты $\eta$ заведомо независимы (и имеют стандартное нормальное распределение), а посему
$$ p_\eta(x) = \prod\limits_{i=1}^n{p_{\eta_i}(x_i)} = \prod\limits_{i=1}^n{\frac{1}{\sqrt{2\pi}} \expon{-\frac{1}{2}x_i^2}} = \frac{1}{(2\pi)^\frac{n}{2}} \cdot\expon{-\frac{1}{2}x^T x}$$
В таком случае, с учетом выражения $\xi$ через $\eta$, получим
$$ p_\xi(x) = \frac{1}{(\det{K})^\frac{1}{2}}\cdot p_\eta(K^{-\frac{1}{2}} (x - a)) = \frac{1}{(2\pi)^\frac{n}{2}}\cdot \frac{1}{(\det{K})^\frac{1}{2}} \expon{-\frac{1}{2} (x-a)^TK^{-1}(x-a)}$$ 

\end{enumerate}
\end{proof}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{15. Распределение (центральное) хи-квадрат Пирсона и лемма о круговом гауссовском распределении.}

\begin{mydef}
Величина $\eta_k$ имеет распределение хи-квадрат Пирсона с $k$ степенями свободы, если $\eta = \sum\limits_{i=1}^k{\xi_i}$, где все $\xi_i$ независимы и имеют стандартное нормальное распределение.
Обозначение: $\eta_k \sim \chi^2(k) $
\end{mydef}

\begin{lemma}[\bf о круговом гауссовском распределении]
Пусть $\xi \sim \mathcal{N}(0, \sigma^2E_n), \sigma^2 > 0$. Тогда 
\begin{enumerate}
\item $\forall C\in O_n(\mathbb{R}) \quad \eta=C\xi\sim\mathcal{N}(0, \sigma^2E_n)$
\item Пусть $L_1, L_2$ --- подпространства в $\mathbb{R}^n$, $L_1 \perp L_2$, тогда $\proj{L_i}{\xi}$ --- независимые гауссовские случайные вектора, $E\proj{L_i}{\xi} = 0$, $\frac{1}{\sigma^2}|\proj{L_i}{\xi}|^2 \sim \chi^2(\dim{L_i})$
\end{enumerate}
\begin{proof}
$ $
\begin{enumerate}
\item $\eta$ --- гаусовский, $E\eta = 0$, $D\eta = E\eta\eta^T = C(\sigma^2E_n)C^T = \sigma^2E_n \Longrightarrow \eta\sim\mathcal{N}(0, \sigma^2E_n)$
\item Пусть $\dim L_1 = p,\,\,\dim L_2 = m$ 
Рассмотрим ОНБ\footnote{Ортонормированный базис} $(e_1,\ldots,e_n)$ в $\mathbb{R}^n$, такой, что $L_1 = \langle e_1,\ldots,e_p\rangle, L_2 = \langle e_{p+1},\ldots,e_{p+m}\rangle$. В этом случае проекции - это просто некоторые из компонент $\xi$. Дальше очевидно.
\end{enumerate}
\end{proof}

\end{lemma}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{16. Линейная гауссовская модель, полные достаточные статистики и оптимальные оценки для среднего и дисперсии.}

$X = (X_1,\ldots,X_n)T \sim \mathcal{N}(l, \sigma^2E_n),\,\sigma>0, \sigma$ неизвестное число, $l\in\mathbb{R}^n$ --- неизвестный вектор. Известно, что $l\in L,\,\,L\subset \mathbb{R}^n$ --- заданное подпространство. $\dim L = p < n$.
Обозначим $\varepsilon = X - l$. В этом случае 
$$X = l + \varepsilon,\,\, \varepsilon\sim\mathcal{N}(0, \sigma^2E_n)$$
$(l^T, \sigma^2)$ --- неизвестный параметр

\begin{mydef}
Полученная статистическая модель называется гауссовской линейной моделью.
\end{mydef}

\noindent {\bf Утверждение. } Полной достаточной статистикой в данной ситуации является $((\proj{L}{X})^T, |\proj{L^\perp}{X}|^2)^T$
\begin{proof}
$$ p(x, l, \sigma^2) = \left(\frac{1}{\sqrt{2\pi}}\right)^n \expon{-\frac{1}{2\sigma^2}|x - l|^2}$$
$$ x = \proj{L}{x} + \proj{L^\perp}{x} \Longrightarrow |x - l|^2 = |\proj{L}{x} - l + \proj{L^\perp}{x}|^2 = |\proj{L}{x} - l|^2 + |\proj{L^\perp}{x}|^2 $$
Таким образом
$$ p(x, l, \sigma^2) = \left(\frac{1}{\sqrt{2\pi}}\right)^n \expon{-\frac{1}{2\sigma^2}\left(|\proj{L}{x} - l|^2 + |\proj{L^\perp}{x}|^2\right)}$$
Получаем достоточность из теоремы факторизации. Полноту оставим без доказательства.
\end{proof}

\noindent Опишем некоторые важные факты о гауссовской модели.
\begin{enumerate}
\item Оптимальная оценка для $l$.
$$ \proj{L}{x} = \proj{L}{l} + \proj{L}{\varepsilon} $$
$E(\proj{L}{\varepsilon}) = 0,\,\, E(\proj{L}{x}) = l$, из чего мы заключаем, что оптимальной оценкой для $l$ является $$\hat{l}_n = \proj{L}{X}$$

\item Оптимальная оценка для $\sigma^2$
$$ \proj{L^\perp}{x} = \proj{L^\perp}{l} + \proj{L^\perp}{\varepsilon} = \proj{L^\perp}{\varepsilon} \Longrightarrow \frac{1}{\sigma^2}|\proj{L^\perp}{X}|^2 = \frac{1}{\sigma^2}|\proj{L^\perp}{\varepsilon}|^2 \sim \chi^2(n-p)$$
$$ E\chi^2(n-p) = n-p \Longrightarrow E(\frac{1}{\sigma^2}|\proj{L^\perp}{\varepsilon}|^2) = n -p \Longrightarrow E(\frac{1}{n-p}|\proj{L^\perp}{\varepsilon}|^2) = \sigma^2$$
Значит, оптимальной оценкой для $\sigma^2$ является
$$ \hat{S}_n^2 = \frac{1}{n-p}|\proj{L^\perp}{X}|^2 $$

\item Независимость $\hat{l}_n$ и $\hat{S}_n^2$

$\proj{L}{x}$ и $\proj{L^\perp}{x}$ независимы $\Longrightarrow$ $\hat{l}_n$ и $\hat{S}_n$ независимы.

Это дает нам вывод о том, что $(\hat{l}_n$,$\hat{S}_n^2)$ --- оптимальная оценка для $(l^T, \sigma^2)$
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{17. Линейная гауссовская регрессия, оценки наименьших квадратов неизвестных параметров и их вычисление. Теорема о свойствах о.н.к. (распределения, независимость, оптимальность). Примеры - гауссовская выборка, однофакторная линейная модель.}

Рассмотрим линейную гауссовскую модель: 
$$X = l + \varepsilon,\,\, \varepsilon\sim\mathcal{N}(0, \sigma^2E_n)$$
$\theta = (l^T, \sigma^2),\,\,\dim\theta = p+1$ --- неизвестный параметр, $l\in L,\,\,\dim L = p < n$

Пусть $Z\in M_{n\times p}$ --- матрица, столбцы которой задают координаты базиса $L$ в некотром фиксированном ОНБ в $\mathbb{R}^n$

\begin{mydef}
$Z$ называется регрессионной матрицей гауссовской линейной модели.
\end{mydef}
\noindent После преобразования $l=Zc,\,\,c=(c_1,\ldots,c_p)\in L$ модлеь принимает вид
$$X = Zc + \varepsilon,\,\, \varepsilon\sim\mathcal{N}(0, \sigma^2E_n)$$
или, иначе, в виде отдельных уравнений
$$ X_i = Z_i^Tc + \varepsilon_i,\quad i=1,\ldots,n,\quad \varepsilon_i\sim\mathcal{N}(0,\sigma^2)$$

\begin{mydef}
Оценкой наименьших квадратов вектора $c$ называется решение $\hat{c}_n$ экстремальной задачи
$$ |X - Zc|^2 = \sum\limits_{i=1}^n{(x_i - Z_i^Tc)^2} \longrightarrow \min\limits_{c\in\mathbb{R}^p}$$
\end{mydef}

Найдем оптимальные оценки
\begin{enumerate}
\item Оценка $c$
$$ Z\hat{c}_n = \proj{L}{X} \Longrightarrow Z\hat{c}_n - X \in L^\perp \Longleftrightarrow Z\hat{c}_n - X \perp L$$
Следовательно
$$ Z^T(X- Z\hat{c}_n) = 0 \Longleftrightarrow Z^TX = Z^TZ\hat{c}_n $$ 
$$ \hat{c}_n = (Z^TZ)^{-1}Z^TX $$
Невырожденность $Z^TZ$ следует из того, что столбцы $Z$ --- базисные в $L$

\item Оценка $\sigma^2$

\noindent По доказанному ранее оптимальной оценкой для $\sigma^2$ является
$$ \hat{S}_n^2 = \frac{1}{n-p}|\proj{L^\perp}{X}|^2 = \frac{1}{n-p}|X - \proj{L}{X}|^2 = \frac{1}{n-p}|X - Z\hat{c}_n|^2 = \frac{1}{n-p} \sum\limits_{i=1}^n{(x_i - Z_i^T\hat{c}_n)^2}$$
$$ \hat{S}_n^2 =  \frac{1}{n-p}|X - Z\hat{c}_n|^2 $$
\end{enumerate}

\begin{theorem}
\begin{enumerate}
\item $\hat{c}_n$ --- оптимальная оценка для $c$, а $S_n^2$ --- для $\sigma^2$
\item $\hat{c}_n\sim\mathcal{N}(c, \sigma^2(Z^TZ)^{-1}),\quad\frac{n - p}{\sigma^2}\sim\chi^2(n - p)$
\item $\hat{c}_n$ и $\hat{S}_n^2$ независимы.
\end{enumerate}
\end{theorem}
\begin{proof}
Докажем утверждения теоремы в обратном порядке :)
\begin{enumerate}
\item Пусть $\hat{l}_n = \proj{L}{X}$. Тогда
$$ \hat{c}_n = (Z^TZ)^{-1}(Z^TZ)\hat{c}_n = (Z^TZ)^{-1}Z^T\proj{L}{X} = (Z^TZ)^{-1}Z^T\hat{l}_n $$
$\hat{l}_n$ и $\hat{S}_n^2$ независимы, а значит $\hat{c}_n$ и $\hat{S}_n^2$ независимы
\item Известно, что $\hat{c}_n$ --- гауссовская.
$$ \hat{c}_n = (Z^TZ)^{-1}Z^TX $$
$$ E\hat{c}_n = (Z^TZ)^{-1}Z^TZc = c$$
$$ D\hat{c}_n = (Z^TZ)^{-1}Z^T(\sigma^2E_n)Z((Z^TZ)^{-1})^T = \sigma^2(Z^TZ)^{-1}$$
Следовательно, $\hat{c}_n\sim\mathcal{N}(c, \sigma^2(Z^TZ)^{-1})$
\item Следует из того, что $ \hat{c}_n$ и $\hat{S}_n^2$ --- функции достаточной статистики.
\end{enumerate}
\end{proof}

\noindent Анализ однофакторных моделей --- дело простое и приятное, но здесь освещен не будет.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{18. Распределение Стьюдента (центральное) и доверительные интервалы для параметров гауссовской выборки.}

\noindent Введем для начала понятие распределений Стьюдента.
\begin{mydef}
Пусть $\xi_0,\ldots,\xi_k$ независимы имеют стандартное нормальное распределение. Тогда, по определению, величина
$$ t_k = \frac{\xi_0}{\sqrt{\frac{1}{k}\sum\limits_{i = 1}^k{\xi_i^2}}} = \frac{\xi_0}{\sqrt{\frac{1}{k}\chi^2(k)}} $$
имеет распределение Стьюдента с $k$ степенями свободы.

Обозначение: $t_k\sim S(k)$
\end{mydef}

\noindent Пусть $X=(X_1,\ldots,X_n)$ --- выборка из распределения $\mathbb{P}_X\in\{\mathbb{P}_\theta:\theta\in\Theta\subseteq\mathbb{R}\}$. $T_1(X)$ и $T_2(X)$ --- статистики, причем $T_1(X) \leq T_2(X),\,\,(T_1; T_2)\footnote{Имеется ввиду интервал, а не пара.}\subseteq\Theta$

\begin{mydef}
Интервал $(T_1(X); T_2(X))$ называется доверительным интервалом уровня доверия $1-\alpha$ для параметра $\theta$, если
$$ \forall\theta\in\Theta\quad \mathbb{P}_\theta(T_1(X) < \theta < T_2(X)) \geq 1-\alpha $$
\end{mydef}

Опишем в некоторых ситуациях доверительные интервалы для параметров гауссовской выборки.
\begin{enumerate}
\item Доверительный интервал для среднего при известной дисперсии

$X=(X_1,\ldots,X_n)$ --- выборка из $\mathcal{N}(a, \sigma^2)$. Оптимальной оценкой для $a$ является $\overline{X} = \frac{1}{n}\sum\limits_{i=1}^n{X_i}$. 
$\overline{X} \sim \mathcal{N}(a, \frac{\sigma^2}{n}) \Longrightarrow \frac{\sqrt{n}(\overline{X} - a)}{\sigma}\sim\mathcal{N}(0,1)$.

\noindent Обозначим через $\xi_\alpha$ $\alpha$-квантиль стандартного нормального распределения. Очевидно, $\xi_\alpha = -\xi_{1 - \alpha}.$ Из симметрии плотности нормального распределния заключаем, что
$$ \mathbb{P}_\alpha\left(\left|\frac{\sqrt{n}(\overline{X} - a)}{\sigma} \right| < \xi_{1 - \frac{\alpha}{2}}\right) = 1 - \alpha \Longleftrightarrow \mathbb{P}_\alpha\left(|\overline{X} - a| < \frac{\sigma\cdot\xi_{1 - \frac{\alpha}{2}}}{\sqrt{n}}\right) = 1 - \alpha $$
Из этого получаем доверительный интервал для $a$ с вероятностью $1-\alpha$
$$ \overline{X} - \frac{\sigma\cdot\xi_{1 - \frac{\alpha}{2}}}{\sqrt{n}} < a < \overline{X} + \frac{\sigma\cdot\xi_{1 - \frac{\alpha}{2}}}{\sqrt{n}}$$
Его длина $$l_n = 2\frac{\sigma\cdot\xi_{1 - \frac{\alpha}{2}}}{\sqrt{n}}$$. О ней можно сказать следующее:
\begin{enumerate}
\item $\alpha\to 0 \Rightarrow l_n\to +\infty$
\item $n \to 0 \Rightarrow l_n\to 0$
\item $\sigma\to 0 \Rightarrow l_n\to 0$
\end{enumerate}

\item Доверительный интервал для среднего при неизвестной дисперсии.

\noindent Обозначим через $t_\alpha(k)$ $\alpha$-квантиль распределения Стьюдента с $k$ степенями свободы. Очевидно, $t_\alpha(k) = -t_{-\alpha}(k)$.

И снова, $\overline{X} \sim \mathcal{N}(a, \frac{\sigma^2}{n}) \Longrightarrow \frac{\sqrt{n}(\overline{X} - a)}{\sigma}\sim\mathcal{N}(0,1)$. Введем
$$ S^2 = \frac{1}{n-1}\sum\limits_{i=1}^n{(X_i - \overline{X})^2}$$
Заметим, что $ES^2 = \sigma^2$, а также $\frac{(n-1)S^2}{\sigma^2} \sim\chi^2(n-1)$. $\overline{X}$ и $S^2$ независимы, а значит
$$ \frac{\frac{\sqrt{n}(\overline{X} - a)}{\sigma}}{\sqrt{\frac{(n-1)S^2}{\sigma^2}\cdot\frac{1}{n-1}}} \sim S(n-1) \Longleftrightarrow \frac{\sqrt{n}(\overline{X} - a)}{S}\sim S(n-1)$$
Таким образом
$$ \mathbb{P}_\alpha\left(\left|\frac{\sqrt{n}(\overline{X} - a)}{S} \right| < t_{1 - \frac{\alpha}{2}}(n-1)\right) > 1 - \alpha \Longleftrightarrow \mathbb{P}_\alpha\left(|\overline{X} - a| < \frac{S\cdot t_{1 - \frac{\alpha}{2}}(n-1)}{\sqrt{n}}\right) > 1 - \alpha $$ 
Получаем доверительный интервал вероятности $1-\alpha$:
$$ \overline{X} - \frac{S\cdot t_{1 - \frac{\alpha}{2}}(n-1)}{\sqrt{n}} < a < \overline{X} + \frac{S\cdot t_{1 - \frac{\alpha}{2}}(n-1)}{\sqrt{n}} $$

\item Доверительный интервал для дисперсии

\noindent Обозначим через $\chi^2_\alpha(n-1)$ $\alpha$-квантиль распределения $\chi^2(n-1)$.

$$ \mathbb{P}_{a, \sigma^2}\left( \chi^2_{\frac{\alpha}{2}}(n-1) < \frac{(n-1)S^2}{\sigma^2} < \chi^2_{1 - \frac{\alpha}{2}}(n-1)\right) = 0$$
Соответственно, интервал уровня доверия $1-\alpha$:
$$ \frac{(n-1)S^2}{\chi^2_{1-\frac{\alpha}{2}}(n-1)} < \sigma^2 < \frac{(n-1)S^2}{\chi^2_{\frac{\alpha}{2}}(n-1)}$$

\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{19. Распределение Фишера (центральное) и доверительные эллипсоиды и интервалы для параметров гауссовской линейной регрессии.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{20. Слабая сходимость случайных векторов, лемма о наследовании слабой сходимости, лемма Слуцкого.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{21. Асимптотически нормальные и состоятельные оценки. Асимптотические доверительные интервалы с асимптотически равномерно наименьшей длиной. Теорема Бахадура (без доказательства) и асимптотически эффективные оценки.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{22. Правдоподобие выборки, теорема об экстремальном свойстве правдоподобия.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{23. Метод максимального правдоподобия. Теорема о состоятельном решении уравнения правдоподобия.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{24. Теорема об асимптотической эффективности состоятельного решения уравнения правдоподобия.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{25. Метод максимального правдоподобия в векторном случае (результаты без доказательства).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{26. Общие понятия теории проверки статистических гипотез: гипотеза (параметрическая и непараметрическая), критическое множество (критерий), ошибки 1-го и 2-го рода, мощность, уровень значимости. Равномерно наиболее мощные и наиболее мощные критерии.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{27. Лемма Неймана-Пирсона и примеры ее применения.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{28. Доверительные множества и теорема о двойственности задач оценивания и проверки гипотез.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{29. Нецентральные распределения хи-квадрат Пирсона, Стьюдента и Фишера. Лемма о свойствах нецентральных распределений (зависимость от параметра нецентральности, стохастическая упорядоченность).}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{30. F-критерий Фишера для проверки линейных гипотез в гауссовской линейной регрессии. Мощность F-критерия, явный вид параметра нецентральности, монотонность по параметру нецентральности, несмещенность.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{31. Проверка однородности двух гауссовских выборок F-критерием.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{32. Проверка простой гипотезы в схеме независимых полиномиальных испытаний критерием хи-квадрат Пирсона, теорема Пирсона.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\subsubsection*{33. Приложение критерия Пирсона к проверке непараметрической гипотезы о виде функции распределения. Сравнение с критерием Колмогорова.}

\end{document}
